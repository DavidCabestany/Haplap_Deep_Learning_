{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCM_3. LSTM based Language Model_DONE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skxNOBVngUuI"
      },
      "source": [
        "# Assignment 3: Language Modelling with LSTM networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od8wz1GsgUuO"
      },
      "source": [
        "In this assignment, you will implement an LSTM based language model. We strongly recommend to finish first _lab 4_, which is closely related and is much simpler."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG0W24LVgUuR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9YkdodygUuW"
      },
      "source": [
        "First, let's load the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqj5ZEmHqZHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ccce439-132e-4a6a-aeab-c85c9dd0661f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhVrZbAAgUuZ"
      },
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# Let's do 2-way positive/negative classification instead of 5-way\n",
        "easy_label_map = {0:0, 1:0, 2:None, 3:1, 4:1}\n",
        "\n",
        "def load_sst_data(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            example = {}\n",
        "            \n",
        "            # Strip out the parse information and the phrase labels---we don't need those here\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "\n",
        "    random.seed(1)\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "\n",
        "sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n",
        "training_set = load_sst_data(sst_home + '/train.txt')\n",
        "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
        "test_set = load_sst_data(sst_home + '/test.txt')\n",
        "\n",
        "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
        "# trim down the dev and test sets. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TAVmSzNmmf-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7vCn9ElgUui"
      },
      "source": [
        "Next, we'll convert the data to index vectors.\n",
        "\n",
        "To simplify your implementation, we'll use a fixed unrolling length of 20. This means that we'll have to expand each sentence into a sequence of 21 word indices. In the conversion process, we'll mark the start of each sentence with a special word symbol `<S>`, mark the end of each sentence (if it occurs within the first 21 words) with a special word symbol `</S>`, mark extra tokens after `</S>` with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNfCXqF8gUul"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "def sentence_to_padded_index_sequence(datasets):\n",
        "    '''Annotates datasets with feature vectors.'''\n",
        "    \n",
        "    START = \"<S>\"\n",
        "    END = \"</S>\"\n",
        "    END_PADDING = \"<PAD>\"\n",
        "    UNKNOWN = \"<UNK>\"\n",
        "    SEQ_LEN = 21\n",
        "    \n",
        "    # Extract vocabulary\n",
        "    def tokenize(string):\n",
        "        return string.lower().split()\n",
        "    \n",
        "    word_counter = collections.Counter()\n",
        "    for example in datasets[0]:\n",
        "        word_counter.update(tokenize(example['text']))\n",
        "    \n",
        "    vocabulary = set([word for word in word_counter if word_counter[word] > 25])\n",
        "    vocabulary = list(vocabulary)\n",
        "    vocabulary = [START, END, END_PADDING, UNKNOWN] + vocabulary\n",
        "        \n",
        "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
        "        \n",
        "    for i, dataset in enumerate(datasets):\n",
        "        for example in dataset:\n",
        "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
        "            \n",
        "            token_sequence = [START] + tokenize(example['text']) + [END]\n",
        "            \n",
        "            for i in range(SEQ_LEN):\n",
        "                if i < len(token_sequence):\n",
        "                    if token_sequence[i] in word_indices:\n",
        "                        index = word_indices[token_sequence[i]]\n",
        "                    else:\n",
        "                        index = word_indices[UNKNOWN]\n",
        "                else:\n",
        "                    index = word_indices[END_PADDING]\n",
        "                example['index_sequence'][i] = index\n",
        "    return indices_to_words, word_indices\n",
        "    \n",
        "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCU1I5qSgUus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69f1f2d7-f737-4889-f976-33c0a3d8582e"
      },
      "source": [
        "print(training_set[18])\n",
        "print(len(word_indices))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': \"It could have been something special , but two things drag it down to mediocrity -- director Clare Peploe 's misunderstanding of Marivaux 's rhythms , and Mira Sorvino 's limitations as a classical actress .\", 'index_sequence': array([  0, 568, 358, 515, 255, 373, 293, 118, 112, 165, 223,   3, 568,\n",
            "       367,  28,   3, 324, 259,   3,   3, 364], dtype=int32)}\n",
            "603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ASkTrIogUu3"
      },
      "source": [
        "## Assignments: \n",
        "### Part 1: Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I__f8rvOgUu5"
      },
      "source": [
        "Now, using the starter code and hyperparameter values provided below, implement an LSTM language model with dropout on the non-recurrent connections. Use the standard form of the LSTM reflected in the slides (without peepholes). **You should only have to edit the marked sections of code to build the base LSTM**, though implementing dropout properly may require small changes to the main training loop and to brittle_sampler().\n",
        "\n",
        "**Don't use any TensorFlow code that is specifically built for RNNs**. If a TF function has 'recurrent', 'sequence', 'LSTM', or 'RNN' in its name, you should built it yourself instead of using it. (Your version will likely be much simpler, by the way, since these built in methods are powerful but fairly complex and potentially confusing.)\n",
        "\n",
        "We won't be evaluating our model in the conventional way (perplexity on a held-out test set) for a few reasons: to save time, because we have no baseline to compare against, and because overfitting the training set is a less immediate concern with these models than it was with sentence classifiers. Instead, we'll use the value of the cost function to make sure that the model is converging as expected, and we'll use samples drawn from the model to qualitatively evaluate it.\n",
        "\n",
        "**Tips**: \n",
        "- Check the code for the GRU based sentiment classifier (lab 4), specially the part where the RNN structure is defined.\n",
        "- You'll need to use `tf.nn.embedding_lookup()`, `tf.nn.sparse_softmax_cross_entropy_with_logits()`, and `tf.split()` at least once each. All three should be easy to Google, though the last homework and the last exercise should show examples of the first two.\n",
        "- As before, you'll want to initialize your trained parameters using something like `tf.random_normal(..., stddev=0.1)`\n",
        "\n",
        "**TODOS:**\n",
        "- **TODO1**: Define the parameters of the LSTM (check the given slides in class)\n",
        "- **TODO2**: Build the LSTM LM (follow the instructions in the code-comments)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKishKAwgUu8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c604b646-9b98-450e-b6d1-290295791048"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.8.0'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o4syEQOgUvG"
      },
      "source": [
        "class LanguageModel:\n",
        "    def __init__(self, vocab_size, sequence_length):\n",
        "        # Define the hyperparameters\n",
        "        self.learning_rate = 0.3  # Should be about right\n",
        "        self.training_epochs = 250  # How long to train for - chosen to fit within class time\n",
        "        self.display_epoch_freq = 1  # How often to test and print out statistics\n",
        "        self.dim = 32  # The dimension of the hidden state of the RNN\n",
        "        self.embedding_dim = 16  # The dimension of the learned word embeddings\n",
        "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
        "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
        "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
        "        self.rate = 0.25  # Used in dropout (at training time only, not at sampling time)\n",
        "        \n",
        "        #### Start main editable code block ####\n",
        "        self.trainable_variables = []\n",
        "        # Your model should populate the following four python lists.\n",
        "        # self.logits should contain one [batch_size, vocab_size]-shaped TF tensor of logits \n",
        "        #   for each of the 20 steps of the model.\n",
        "        # self.costs should contain one [batch_size]-shaped TF tensor of cross-entropy loss \n",
        "        #   values for each of the 20 steps of the model.\n",
        "        # self.h and c should each start contain one [batch_size, dim]-shaped TF tensor of LSTM\n",
        "        #   activations for each of the 21 *states* of the model -- one tensor of zeros for the \n",
        "        #   starting state followed by one tensor each for the remaining 20 steps.\n",
        "        # Don't rename any of these variables or change their purpose -- they'll be needed by the\n",
        "        # pre-built sampler.\n",
        "        \n",
        "        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.E)\n",
        "        \n",
        "        # 1: Forget\n",
        "        self.W_f = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_f = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_f)\n",
        "        self.trainable_variables.append(self.b_f)\n",
        "        \n",
        "        # 2: Decide and prepare new information\n",
        "        self.W_i = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_i = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_i)\n",
        "        self.trainable_variables.append(self.b_i)\n",
        "\n",
        "        self.W_c = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_c = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_c)\n",
        "        self.trainable_variables.append(self.b_c)\n",
        "\n",
        "        # 4: Decide and prepare output\n",
        "        self.W_o = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_o = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_o)\n",
        "        self.trainable_variables.append(self.b_o)\n",
        "\n",
        "        \n",
        "        # 5: Output\n",
        "        self.W_cost = tf.Variable(tf.random.normal([self.dim, self.vocab_size], stddev=0.1))\n",
        "        self.b_cost = tf.Variable(tf.random.normal([self.vocab_size], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_cost)\n",
        "        self.trainable_variables.append(self.b_cost)\n",
        "         \n",
        "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
        "        self.c_zero = tf.zeros([self.batch_size, self.dim])\n",
        "        \n",
        "        \n",
        "    \n",
        "    def model(self,x,rate,sample=False,h_zero=None,c_zero=None):\n",
        "        def step(x, next_x, h_prev, c_prev):\n",
        "           #TODO\n",
        "          # Get embedding\n",
        "          emb = tf.nn.embedding_lookup(params=self.E,ids=x)\n",
        "          emb_h_prev = tf.concat([emb, tf.squeeze (h_prev)], 1)\n",
        "\n",
        "         \n",
        "          f_t = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_f)  + self.b_f) # Forget\n",
        "          i_t = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_i)  + self.b_i)  # Decide\n",
        "          c_t_hat = tf.nn.tanh(tf.matmul(emb_h_prev, self.W_c)  + self.b_c) # Prepare\n",
        "          c_prev = tf.math.multiply(f_t,c_prev) + tf.math.multiply(i_t,c_t_hat)  # Update\n",
        "          o_t = tf.nn.sigmoid(tf.matmul(emb_h_prev, self.W_o)  + self.b_o) # Decide output\n",
        "          h_prev = tf.math.multiply(o_t,tf.nn.tanh(c_prev))\n",
        "\n",
        "          logits = tf.squeeze(tf.matmul(h_prev, self.W_cost) + self.b_cost)\n",
        "          costs = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=next_x, logits=logits)\n",
        "               \n",
        "          return logits, costs, h_prev, c_prev\n",
        "\n",
        "        self.x_slices = tf.split(x, self.sequence_length, 1)\n",
        "        all_logits = []\n",
        "        all_costs = []\n",
        "        \n",
        "        if h_zero != None and c_zero != None:\n",
        "          self.h = [h_zero]\n",
        "          self.c = [c_zero]\n",
        "        else:\n",
        "          self.h = [self.h_zero]\n",
        "          self.c = [self.c_zero]\n",
        "        \n",
        "        x = tf.reshape(self.x_slices[0], [-1])\n",
        "\n",
        "        #### End main editable code block ####\n",
        "        \n",
        "                \n",
        "        #TODO unroll\n",
        "\n",
        "        if sample: #Para evaluar el modelo\n",
        "            x_sample_next = tf.reshape(self.x_slices[1], [-1])\n",
        "            logits, _, h_sample, c_sample = step(x, x_sample_next, self.h, self.c)\n",
        "            return h_sample, c_sample, logits\n",
        "\n",
        "\n",
        "        for t in range(self.sequence_length-1):\n",
        "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
        "            x_t_next = tf.reshape(self.x_slices[t + 1], [-1])\n",
        "            logits, costs, h_prev, c_prev = step(x_t, x_t_next, self.h, self.c)\n",
        "            all_logits.append(logits)\n",
        "            all_costs.append(costs)\n",
        "\n",
        "        return all_logits,all_costs\n",
        "                       \n",
        "\n",
        "    def train(self, training_data):\n",
        "        def get_minibatch(dataset, start_index, end_index):\n",
        "            indices = range(start_index, end_index)\n",
        "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
        "            return vectors\n",
        "        \n",
        "       \n",
        "        print('Training.')\n",
        "\n",
        "        # Training cycle\n",
        "        for epoch in range(self.training_epochs):\n",
        "            random.shuffle(training_set)\n",
        "            avg_cost = 0.\n",
        "            total_batch = int(len(training_set) / self.batch_size)\n",
        "            \n",
        "            # Loop over all batches in epoch\n",
        "            for i in range(total_batch):\n",
        "                # Assemble a minibatch of the next B examples\n",
        "                minibatch_vectors = np.int32(get_minibatch(training_set, self.batch_size * i, self.batch_size * (i + 1)))\n",
        "\n",
        "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
        "                # cost function for logging\n",
        "                with tf.GradientTape() as tape:\n",
        "                  logits,costs = self.model(minibatch_vectors,self.rate)\n",
        "                  # Sum costs for each word in each example, but average cost across examples.\n",
        "                  costs_tensor = tf.concat([tf.expand_dims(cost, 1) for cost in costs], 1)\n",
        "                  cost_per_example = tf.reduce_sum(costs_tensor, 1)\n",
        "                  total_cost = tf.reduce_mean(cost_per_example)\n",
        "            \n",
        "                # This performs the main SGD update equation\n",
        "                gradients = tape.gradient(total_cost, self.trainable_variables)\n",
        "                optimizer = tf.optimizers.SGD(self.learning_rate)\n",
        "                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "                                                            \n",
        "                # Compute average loss\n",
        "                avg_cost += total_cost / (total_batch * self.batch_size)\n",
        "                \n",
        "            # Display some statistics about the step\n",
        "            if (epoch+1) % self.display_epoch_freq == 0:\n",
        "                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \"Sample:\", self.sample())\n",
        "    \n",
        "    def sample(self):\n",
        "        # This samples a sequence of tokens from the model starting with <S>.\n",
        "        # We only ever run the first timestep of the model, and use an effective batch size of one\n",
        "        # but we leave the model unrolled for multiple steps, and use the full batch size to simplify \n",
        "        # the training code. This slows things down.\n",
        "\n",
        "        def brittle_sampler():\n",
        "            # The main sampling code. Can fail randomly due to rounding errors that yield probibilities\n",
        "            # that don't sum to one.\n",
        "            \n",
        "            word_indices = [0] # 0 here is the \"<S>\" symbol\n",
        "            for i in range(self.sequence_length - 1):\n",
        "                dummy_x = np.zeros((self.batch_size, self.sequence_length),dtype=np.int32)\n",
        "                dummy_x[0][0] = word_indices[-1]\n",
        "                model_h = None\n",
        "                model_c = None\n",
        "                if i > 0:\n",
        "                    model_h = h\n",
        "                    model_c = c\n",
        "                h, c, logits = self.model(dummy_x,0.0,sample=True,h_zero=model_h,c_zero=model_c)\n",
        "                logits = logits[0, :] # Discard all but first batch entry\n",
        "                exp_logits = np.exp(logits - np.max(logits))\n",
        "                distribution = exp_logits / exp_logits.sum()\n",
        "                sampled_index = np.flatnonzero(np.random.multinomial(1, distribution))[0]\n",
        "                word_indices.append(sampled_index)\n",
        "            words = [indices_to_words[index] for index in word_indices]\n",
        "            return ' '.join(words)\n",
        "        \n",
        "        while True:\n",
        "            try:\n",
        "              sample = brittle_sampler()\n",
        "              return sample\n",
        "            except ValueError as e:  # Retry if we experience a random failure.\n",
        "              pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7-7pfUSgUvN"
      },
      "source": [
        "Now let's train it.\n",
        "\n",
        "Once you're confident your model is doing what you want, let it run for the full 250 epochs. This will take some time—likely between five and thirty minutes. If it much longer on a reasonably modern laptop—more than an hour—that suggests serious problems with your implementation. A properly implemented model with dropout should reach an average cost of less than 0.22 quickly, and then slowly improve from there. We train the model for a fairly long time because these small improvements in cost correspond to fairly large improvements in sample quality.\n",
        "\n",
        "Samples from a trained models should have coherent portions, but they will not resemble interpretable English sentences. Here are three examples from a model with a cost value of 0.202:\n",
        "\n",
        "`<S> the good <UNK> and <UNK> and <UNK> <UNK> with predictable and <UNK> , but also does one of -lrb- <UNK>`\n",
        "\n",
        "`<S> <UNK> has <UNK> actors seems done <UNK> would these <UNK> <UNK> to <UNK> <UNK> <UNK> 're <UNK> to mind .`\n",
        "\n",
        "`<S> an action story that was because the <UNK> <UNK> are when <UNK> as ``` <UNK> '' ' it is any`\n",
        "\n",
        "`-lrb-` and `-rrb` are the way that left and right parentheses are represented in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zifuF0xQgUvR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b4fd7b-a3ad-4bc2-8d23-efb983028027"
      },
      "source": [
        "model = LanguageModel(len(word_indices), 21)\n",
        "model.train(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training.\n",
            "Epoch: 1 Cost: 0.312654316 Sample: <S> <UNK> of days and point of though animation a <UNK> . of <UNK> 's found flat . <UNK> <UNK> interesting\n",
            "Epoch: 2 Cost: 0.264638752 Sample: <S> '' shot sense and earnest believe a in wit and that 's <UNK> filmmakers powerful <UNK> . and you 's\n",
            "Epoch: 3 Cost: 0.255139202 Sample: <S> <UNK> is looking to <UNK> <UNK> <UNK> it <UNK> piece seen that n't this 's with <UNK> has kids a\n",
            "Epoch: 4 Cost: 0.248993322 Sample: <S> the with its <UNK> <UNK> as <UNK> and funny and a from that had , to the <UNK> romantic in\n",
            "Epoch: 5 Cost: 0.245033622 Sample: <S> if the us <UNK> rare <UNK> as clever moment <UNK> of the that of <UNK> intelligence of <UNK> something <UNK>\n",
            "Epoch: 6 Cost: 0.241364628 Sample: <S> an <UNK> beyond place from none what . 's falls n't but a <UNK> for <UNK> and what on a\n",
            "Epoch: 7 Cost: 0.237940043 Sample: <S> ride , but -lrb- in a a <UNK> <UNK> and all the classic us <UNK> from <UNK> , <UNK> a\n",
            "Epoch: 8 Cost: 0.235706523 Sample: <S> next <UNK> <UNK> and and a <UNK> <UNK> , and quite a <UNK> and <UNK> <UNK> <UNK> and too <UNK>\n",
            "Epoch: 9 Cost: 0.233916417 Sample: <S> the <UNK> call that <UNK> <UNK> the <UNK> <UNK> 's ... ... about all the <UNK> <UNK> but that 's\n",
            "Epoch: 10 Cost: 0.232312918 Sample: <S> if it have to <UNK> powerful that all a <UNK> tragedy , <UNK> the <UNK> <UNK> , but and <UNK>\n",
            "Epoch: 11 Cost: 0.23154287 Sample: <S> debut to a <UNK> <UNK> <UNK> , and and probably it n't enough of worst <UNK> <UNK> for his <UNK>\n",
            "Epoch: 12 Cost: 0.230073631 Sample: <S> the hard <UNK> . movie n't are soap director n't <UNK> would <UNK> <UNK> about as a does <UNK> <UNK>\n",
            "Epoch: 13 Cost: 0.229095593 Sample: <S> the thought , <UNK> and <UNK> if it to be the film <UNK> by <UNK> moments who . </S> <PAD>\n",
            "Epoch: 14 Cost: 0.227883711 Sample: <S> a <UNK> of <UNK> the look if too serious <UNK> <UNK> , <UNK> <UNK> <UNK> -rrb- of <UNK> at the\n",
            "Epoch: 15 Cost: 0.226831272 Sample: <S> point of portrait , <UNK> to people his <UNK> and that <UNK> from <UNK> to a <UNK> <UNK> , and\n",
            "Epoch: 16 Cost: 0.225968435 Sample: <S> the funny , <UNK> <UNK> did and <UNK> you with summer few . n't really to very recent <UNK> is\n",
            "Epoch: 17 Cost: 0.22524856 Sample: <S> <UNK> <UNK> <UNK> , and as a <UNK> people of modern <UNK> and <UNK> <UNK> that so instead . </S>\n",
            "Epoch: 18 Cost: 0.224518552 Sample: <S> and the dramatic <UNK> in <UNK> by <UNK> like the pleasure and , about when this cast and an <UNK>\n",
            "Epoch: 19 Cost: 0.224239618 Sample: <S> <UNK> hour , and i <UNK> hollywood of <UNK> big <UNK> of want it never have to <UNK> in <UNK>\n",
            "Epoch: 20 Cost: 0.223511502 Sample: <S> <UNK> up is on <UNK> <UNK> <UNK> from visual death good , `` the <UNK> cliches of <UNK> <UNK> ,\n",
            "Epoch: 21 Cost: 0.223356202 Sample: <S> a <UNK> not half an <UNK> <UNK> <UNK> director of the some power is could much <UNK> and and director\n",
            "Epoch: 22 Cost: 0.222336099 Sample: <S> an <UNK> <UNK> of documentary . , cinema or back and and at entertainment <UNK> 's to always <UNK> <UNK>\n",
            "Epoch: 23 Cost: 0.22198078 Sample: <S> ... it 'll many only <UNK> <UNK> is we <UNK> <UNK> in this <UNK> and performances who has an <UNK>\n",
            "Epoch: 24 Cost: 0.221449986 Sample: <S> it `` <UNK> of a <UNK> <UNK> <UNK> <UNK> and and , will least a far <UNK> funny , and\n",
            "Epoch: 25 Cost: 0.221142158 Sample: <S> <UNK> more on <UNK> <UNK> <UNK> its <UNK> is , <UNK> tale , but it not that just also <UNK>\n",
            "Epoch: 26 Cost: 0.220779166 Sample: <S> <UNK> and <UNK> in a <UNK> how to a <UNK> <UNK> or and <UNK> feature to political <UNK> <UNK> <UNK>\n",
            "Epoch: 27 Cost: 0.22056739 Sample: <S> a <UNK> <UNK> with <UNK> <UNK> <UNK> yet debut man and <UNK> , part of why <UNK> <UNK> comedy of\n",
            "Epoch: 28 Cost: 0.220508531 Sample: <S> a lot <UNK> <UNK> boy is his <UNK> <UNK> is , <UNK> are all the piece of that the movie\n",
            "Epoch: 29 Cost: 0.220039546 Sample: <S> the director <UNK> a <UNK> <UNK> , there is found that has worth his <UNK> certainly <UNK> to make some\n",
            "Epoch: 30 Cost: 0.219153851 Sample: <S> <UNK> is <UNK> <UNK> anything to have might really too a <UNK> <UNK> <UNK> <UNK> <UNK> 's , <UNK> of\n",
            "Epoch: 31 Cost: 0.21885097 Sample: <S> more <UNK> and it <UNK> movies and , it is an <UNK> , but we might be a most good\n",
            "Epoch: 32 Cost: 0.21896337 Sample: <S> never one , but not almost as a <UNK> <UNK> <UNK> for the film , delivers <UNK> . </S> <PAD>\n",
            "Epoch: 33 Cost: 0.218226925 Sample: <S> <UNK> <UNK> , you could be between <UNK> the <UNK> <UNK> of <UNK> and is a <UNK> old <UNK> often\n",
            "Epoch: 34 Cost: 0.218376338 Sample: <S> it will not <UNK> another movies , but it off to <UNK> contrived , and though out in a <UNK>\n",
            "Epoch: 35 Cost: 0.217914268 Sample: <S> i <UNK> <UNK> 's to subject <UNK> a <UNK> <UNK> is on <UNK> a <UNK> writing ii , should <UNK>\n",
            "Epoch: 36 Cost: 0.217586845 Sample: <S> -lrb- <UNK> production as <UNK> a <UNK> <UNK> who cliches into a <UNK> <UNK> ever <UNK> <UNK> of . </S>\n",
            "Epoch: 37 Cost: 0.217264131 Sample: <S> one then through its <UNK> <UNK> at the <UNK> picture that are <UNK> <UNK> moving , a <UNK> <UNK> ,\n",
            "Epoch: 38 Cost: 0.217107788 Sample: <S> <UNK> <UNK> as for , it was for ` a good good comedy , <UNK> and it <UNK> works is\n",
            "Epoch: 39 Cost: 0.216929913 Sample: <S> this <UNK> <UNK> to get a <UNK> cinematic <UNK> as <UNK> <UNK> the women matter has <UNK> over <UNK> a\n",
            "Epoch: 40 Cost: 0.216557845 Sample: <S> <UNK> most of work are <UNK> , and <UNK> <UNK> , and its <UNK> <UNK> : as the <UNK> himself\n",
            "Epoch: 41 Cost: 0.216369525 Sample: <S> a film <UNK> to <UNK> <UNK> without <UNK> overall , <UNK> and <UNK> <UNK> , but it has a <UNK>\n",
            "Epoch: 42 Cost: 0.216121346 Sample: <S> well as <UNK> <UNK> in <UNK> it 's as -rrb- down the <UNK> : the <UNK> <UNK> of actor .\n",
            "Epoch: 43 Cost: 0.216096178 Sample: <S> if it lacks about the jokes , <UNK> , you be <UNK> that 's n't no most <UNK> with <UNK>\n",
            "Epoch: 44 Cost: 0.215760902 Sample: <S> funny and with this one , <UNK> and one like <UNK> on <UNK> <UNK> <UNK> of an <UNK> the <UNK>\n",
            "Epoch: 45 Cost: 0.215633675 Sample: <S> one up by <UNK> , the new <UNK> , the most <UNK> as <UNK> , girl and <UNK> instead of\n",
            "Epoch: 46 Cost: 0.215466827 Sample: <S> a <UNK> <UNK> film with an <UNK> at those and <UNK> as its <UNK> might be <UNK> -- as a\n",
            "Epoch: 47 Cost: 0.215390146 Sample: <S> a great whole performances but <UNK> , it <UNK> in , but that it seem <UNK> 's enough to <UNK>\n",
            "Epoch: 48 Cost: 0.215086117 Sample: <S> what it 's <UNK> 's enough to most <UNK> <UNK> simply a film some moments . quite . </S> <PAD>\n",
            "Epoch: 49 Cost: 0.214961231 Sample: <S> the <UNK> <UNK> and but it that might make a high characters , <UNK> and , <UNK> <UNK> <UNK> ,\n",
            "Epoch: 50 Cost: 0.214911595 Sample: <S> <UNK> and <UNK> in in you find the human <UNK> <UNK> is more of <UNK> <UNK> , <UNK> , and\n",
            "Epoch: 51 Cost: 0.214677945 Sample: <S> the <UNK> <UNK> and <UNK> , and yet not a <UNK> as a <UNK> <UNK> to <UNK> little <UNK> more\n",
            "Epoch: 52 Cost: 0.214539856 Sample: <S> <UNK> at the <UNK> the <UNK> place to never <UNK> told to watch <UNK> <UNK> , the <UNK> crime to\n",
            "Epoch: 53 Cost: 0.214431778 Sample: <S> a fairly <UNK> <UNK> and <UNK> is that you <UNK> a <UNK> , sad and rather than a such <UNK>\n",
            "Epoch: 54 Cost: 0.214403287 Sample: <S> like an <UNK> <UNK> into as ` <UNK> that <UNK> <UNK> , and <UNK> , both at i much <UNK>\n",
            "Epoch: 55 Cost: 0.213859186 Sample: <S> the lack <UNK> 's as the screenplay <UNK> and <UNK> , and <UNK> <UNK> <UNK> , and <UNK> best and\n",
            "Epoch: 56 Cost: 0.214006037 Sample: <S> the as like a dark <UNK> , <UNK> <UNK> <UNK> , but <UNK> <UNK> in himself it <UNK> is <UNK>\n",
            "Epoch: 57 Cost: 0.213530377 Sample: <S> told for its <UNK> , and <UNK> and <UNK> on its action familiar and interesting dull <UNK> and in this\n",
            "Epoch: 58 Cost: 0.213751242 Sample: <S> ... well so they were for a story <UNK> animation and , bad but the <UNK> <UNK> , while finally\n",
            "Epoch: 59 Cost: 0.21333082 Sample: <S> <UNK> for the sweet narrative time , and that he is n't that i ca n't year <UNK> , it\n",
            "Epoch: 60 Cost: 0.213514984 Sample: <S> it <UNK> does you would have on <UNK> ' , which <UNK> to <UNK> made and laughs this is that\n",
            "Epoch: 61 Cost: 0.213251442 Sample: <S> it works in a sad <UNK> , about the production <UNK> in that it 's no <UNK> <UNK> of <UNK>\n",
            "Epoch: 62 Cost: 0.213212892 Sample: <S> no <UNK> , and <UNK> <UNK> is <UNK> an <UNK> <UNK> of <UNK> , your <UNK> this movie about <UNK>\n",
            "Epoch: 63 Cost: 0.212981373 Sample: <S> <UNK> <UNK> in the film <UNK> <UNK> of <UNK> a hilarious sweet , and director a <UNK> <UNK> . </S>\n",
            "Epoch: 64 Cost: 0.212894112 Sample: <S> <UNK> years and storytelling , <UNK> and , and by its characters it goes , and the film <UNK> <UNK>\n",
            "Epoch: 65 Cost: 0.213037908 Sample: <S> <UNK> from <UNK> <UNK> to own <UNK> and <UNK> , <UNK> , in a <UNK> <UNK> was <UNK> to <UNK>\n",
            "Epoch: 66 Cost: 0.212814778 Sample: <S> <UNK> again , <UNK> <UNK> 's a <UNK> , and <UNK> of movie that , <UNK> you are enough to\n",
            "Epoch: 67 Cost: 0.21272628 Sample: <S> -lrb- <UNK> <UNK> <UNK> between me in the most <UNK> <UNK> and <UNK> of a <UNK> good film . </S>\n",
            "Epoch: 68 Cost: 0.212727025 Sample: <S> <UNK> like a <UNK> the art <UNK> in you would know entirely romance that most <UNK> <UNK> its <UNK> ...\n",
            "Epoch: 69 Cost: 0.212732017 Sample: <S> <UNK> , <UNK> any entertainment of <UNK> of <UNK> ideas and <UNK> whether and <UNK> <UNK> 's performance -- of\n",
            "Epoch: 70 Cost: 0.212420821 Sample: <S> a new <UNK> and <UNK> , despite an much interesting face <UNK> in the <UNK> romantic ii is serious <UNK>\n",
            "Epoch: 71 Cost: 0.212429479 Sample: <S> made by the <UNK> <UNK> and , <UNK> the pretentious in <UNK> of <UNK> and <UNK> , and <UNK> was\n",
            "Epoch: 72 Cost: 0.212117106 Sample: <S> a overall <UNK> and it -rrb- in the mood as a film <UNK> <UNK> of <UNK> and <UNK> <UNK> ,\n",
            "Epoch: 73 Cost: 0.212107033 Sample: <S> it <UNK> 's a bad <UNK> on its <UNK> <UNK> for a bad <UNK> , what getting the most fascinating\n",
            "Epoch: 74 Cost: 0.212016821 Sample: <S> it <UNK> in <UNK> the kids <UNK> 's very new , it : there <UNK> comic has seen on a\n",
            "Epoch: 75 Cost: 0.212000728 Sample: <S> every <UNK> serious and spirit but the good <UNK> scenes to need to call the <UNK> <UNK> <UNK> . </S>\n",
            "Epoch: 76 Cost: 0.211828619 Sample: <S> a <UNK> himself is to <UNK> this is difficult really that director <UNK> to <UNK> <UNK> <UNK> drama of the\n",
            "Epoch: 77 Cost: 0.211990029 Sample: <S> a <UNK> enough and <UNK> old and <UNK> by it just <UNK> <UNK> to <UNK> to <UNK> <UNK> film is\n",
            "Epoch: 78 Cost: 0.211666778 Sample: <S> while <UNK> 's <UNK> the story teen , to <UNK> ... whether and a <UNK> <UNK> . </S> <PAD> <PAD>\n",
            "Epoch: 79 Cost: 0.211513072 Sample: <S> while he will a <UNK> <UNK> on the <UNK> a <UNK> <UNK> and <UNK> <UNK> , are <UNK> <UNK> and\n",
            "Epoch: 80 Cost: 0.211562917 Sample: <S> -lrb- earnest about <UNK> than they in <UNK> comedy , as it my little <UNK> of the characters <UNK> <UNK>\n",
            "Epoch: 81 Cost: 0.211338833 Sample: <S> an <UNK> <UNK> , he <UNK> 's family story , with a <UNK> , <UNK> but emotional <UNK> on film\n",
            "Epoch: 82 Cost: 0.211130768 Sample: <S> the cold <UNK> and , he <UNK> is to <UNK> to the end two women in that , <UNK> <UNK>\n",
            "Epoch: 83 Cost: 0.211233914 Sample: <S> if they may not make it a <UNK> on <UNK> , the place , <UNK> <UNK> of <UNK> , <UNK>\n",
            "Epoch: 84 Cost: 0.211343855 Sample: <S> one both director of <UNK> our black idea of <UNK> a movie me with so easy enough for black <UNK>\n",
            "Epoch: 85 Cost: 0.211157486 Sample: <S> the narrative one thing in <UNK> us , he has because it <UNK> , not <UNK> that , i <UNK>\n",
            "Epoch: 86 Cost: 0.211074591 Sample: <S> predictable <UNK> the <UNK> and <UNK> , you could seem . n't in a <UNK> on the <UNK> <UNK> cinematic\n",
            "Epoch: 87 Cost: 0.210922897 Sample: <S> a lot <UNK> <UNK> , but they might could <UNK> <UNK> , the <UNK> comic <UNK> than and less as\n",
            "Epoch: 88 Cost: 0.210895061 Sample: <S> a light <UNK> next satire is as a <UNK> <UNK> of <UNK> 's a <UNK> of any <UNK> , <UNK>\n",
            "Epoch: 89 Cost: 0.210770234 Sample: <S> a <UNK> this film is with its full , <UNK> <UNK> a story , ' and a <UNK> <UNK> film\n",
            "Epoch: 90 Cost: 0.21090284 Sample: <S> the <UNK> will <UNK> , and <UNK> is about <UNK> of the film <UNK> of film that made you <UNK>\n",
            "Epoch: 91 Cost: 0.210988581 Sample: <S> these every good way , with the <UNK> <UNK> to <UNK> and <UNK> <UNK> of <UNK> out of a great\n",
            "Epoch: 92 Cost: 0.210650802 Sample: <S> <UNK> with all of a <UNK> <UNK> , <UNK> has their <UNK> do , this is <UNK> , <UNK> way\n",
            "Epoch: 93 Cost: 0.21070902 Sample: <S> it even if you in a <UNK> <UNK> <UNK> <UNK> , in <UNK> , though its <UNK> exercise , the\n",
            "Epoch: 94 Cost: 0.21070905 Sample: <S> you to as a <UNK> is <UNK> <UNK> film of <UNK> <UNK> , it 's pretty entertaining quite little by\n",
            "Epoch: 95 Cost: 0.210444778 Sample: <S> a <UNK> without <UNK> , to <UNK> , <UNK> work as all is the mood version else enough in the\n",
            "Epoch: 96 Cost: 0.210369229 Sample: <S> a fine <UNK> for to the <UNK> people rather for to <UNK> film that could <UNK> of the <UNK> <UNK>\n",
            "Epoch: 97 Cost: 0.210482851 Sample: <S> a call by bad <UNK> <UNK> -lrb- , <UNK> of <UNK> are <UNK> , , deeply it for <UNK> is\n",
            "Epoch: 98 Cost: 0.210478619 Sample: <S> visually and the <UNK> <UNK> <UNK> from a <UNK> <UNK> ; its <UNK> <UNK> <UNK> that has <UNK> <UNK> like\n",
            "Epoch: 99 Cost: 0.210221842 Sample: <S> a <UNK> <UNK> in a <UNK> and <UNK> <UNK> film that 's n't , but it is like a <UNK>\n",
            "Epoch: 100 Cost: 0.210322186 Sample: <S> though he <UNK> <UNK> ... with a <UNK> ... satire that <UNK> , from <UNK> <UNK> of a touching ,\n",
            "Epoch: 101 Cost: 0.210022077 Sample: <S> director or <UNK> <UNK> <UNK> - and films from which , what <UNK> about and <UNK> <UNK> <UNK> ii show\n",
            "Epoch: 102 Cost: 0.210016266 Sample: <S> the <UNK> more than it a very small <UNK> you 'll be time : , <UNK> , it is <UNK>\n",
            "Epoch: 103 Cost: 0.210098222 Sample: <S> <UNK> is there from <UNK> , and <UNK> its cast is <UNK> <UNK> , a <UNK> <UNK> . </S> <PAD>\n",
            "Epoch: 104 Cost: 0.209944412 Sample: <S> what does there 's -rrb- <UNK> <UNK> , as <UNK> <UNK> and <UNK> of emotionally <UNK> formula . </S> <PAD>\n",
            "Epoch: 105 Cost: 0.209897727 Sample: <S> especially if you they 're a <UNK> <UNK> '' and <UNK> to keep his <UNK> for <UNK> , <UNK> ,\n",
            "Epoch: 106 Cost: 0.209747195 Sample: <S> <UNK> and an <UNK> <UNK> and <UNK> it 's a <UNK> as a <UNK> <UNK> ... and a <UNK> <UNK>\n",
            "Epoch: 107 Cost: 0.209686428 Sample: <S> the <UNK> <UNK> and <UNK> <UNK> to this is <UNK> of the last place out in the emotionally <UNK> <UNK>\n",
            "Epoch: 108 Cost: 0.209731698 Sample: <S> the hilarious <UNK> <UNK> , offers <UNK> 's around without -lrb- a <UNK> without work in family story than to\n",
            "Epoch: 109 Cost: 0.209528148 Sample: <S> it does believe like a surprising film need in his <UNK> thriller it <UNK> <UNK> an emotional film . </S>\n",
            "Epoch: 110 Cost: 0.209452987 Sample: <S> a little as well at their <UNK> the movie hilarious and more <UNK> <UNK> comedy of -- <UNK> <UNK> .\n",
            "Epoch: 111 Cost: 0.209474683 Sample: <S> made with a <UNK> <UNK> <UNK> , but is the different barely <UNK> of its <UNK> and <UNK> . </S>\n",
            "Epoch: 112 Cost: 0.209452406 Sample: <S> becomes a better <UNK> <UNK> and a movie <UNK> amusing , as do political it <UNK> <UNK> man 's performance\n",
            "Epoch: 113 Cost: 0.209149361 Sample: <S> the <UNK> <UNK> documentary , but ultimately , and love <UNK> <UNK> , particularly <UNK> the film , it 's\n",
            "Epoch: 114 Cost: 0.209483132 Sample: <S> <UNK> themselves with is a <UNK> <UNK> and <UNK> the work of a <UNK> the <UNK> <UNK> <UNK> that 's\n",
            "Epoch: 115 Cost: 0.209507585 Sample: <S> though it <UNK> <UNK> 's <UNK> of humor that is in a <UNK> <UNK> to <UNK> <UNK> 's <UNK> <UNK>\n",
            "Epoch: 116 Cost: 0.209219545 Sample: <S> a <UNK> has none of an original or two other <UNK> <UNK> , you and <UNK> , of <UNK> of\n",
            "Epoch: 117 Cost: 0.209347606 Sample: <S> great <UNK> , but <UNK> <UNK> and than the <UNK> by a <UNK> <UNK> to <UNK> and <UNK> , is\n",
            "Epoch: 118 Cost: 0.209214509 Sample: <S> a premise in <UNK> to be the film , <UNK> and his script `` a movie <UNK> <UNK> <UNK> .\n",
            "Epoch: 119 Cost: 0.209210575 Sample: <S> once you in a world that <UNK> , and though it is to have <UNK> <UNK> after you make to\n",
            "Epoch: 120 Cost: 0.208949268 Sample: <S> <UNK> and moving and about time the portrait of these <UNK> and <UNK> amusing and <UNK> the video energy <UNK>\n",
            "Epoch: 121 Cost: 0.209304482 Sample: <S> can <UNK> , as the real <UNK> who <UNK> is rather than he to keep us a <UNK> do n't\n",
            "Epoch: 122 Cost: 0.20902887 Sample: <S> it nothing <UNK> a <UNK> in <UNK> his characters , and <UNK> '' is - a <UNK> <UNK> , and\n",
            "Epoch: 123 Cost: 0.209002182 Sample: <S> as a <UNK> <UNK> <UNK> , it i <UNK> a ` <UNK> <UNK> death to <UNK> to <UNK> directed and\n",
            "Epoch: 124 Cost: 0.208742604 Sample: <S> it <UNK> look <UNK> you in cinema , with ` it all that its <UNK> you might have a far\n",
            "Epoch: 125 Cost: 0.208965436 Sample: <S> with <UNK> enough on engaging , and <UNK> his story that it is pretty <UNK> <UNK> <UNK> as <UNK> ,\n",
            "Epoch: 126 Cost: 0.20888938 Sample: <S> some well particularly <UNK> as it , it of a <UNK> storytelling <UNK> <UNK> romantic <UNK> . </S> <PAD> <PAD>\n",
            "Epoch: 127 Cost: 0.208696812 Sample: <S> <UNK> of its <UNK> <UNK> 's as a far good thriller or if you other <UNK> is that if everyone\n",
            "Epoch: 128 Cost: 0.208887622 Sample: <S> a <UNK> `` <UNK> better <UNK> in the <UNK> strange movie <UNK> with <UNK> around to a genre <UNK> hollywood\n",
            "Epoch: 129 Cost: 0.20851931 Sample: <S> <UNK> thriller about the whole <UNK> <UNK> enough , and <UNK> <UNK> 's <UNK> if <UNK> funny . </S> <PAD>\n",
            "Epoch: 130 Cost: 0.208629802 Sample: <S> this <UNK> <UNK> and <UNK> in this very funny and not be <UNK> and its characters <UNK> and his is\n",
            "Epoch: 131 Cost: 0.208548322 Sample: <S> the <UNK> <UNK> , <UNK> and <UNK> <UNK> that makes its <UNK> <UNK> or as <UNK> and slow -- as\n",
            "Epoch: 132 Cost: 0.208876297 Sample: <S> the <UNK> and <UNK> cast , <UNK> far from both <UNK> and <UNK> <UNK> <UNK> <UNK> of sad <UNK> <UNK>\n",
            "Epoch: 133 Cost: 0.208678439 Sample: <S> a enough <UNK> while made it <UNK> -rrb- , and which <UNK> an <UNK> <UNK> , although the <UNK> <UNK>\n",
            "Epoch: 134 Cost: 0.208518222 Sample: <S> ... a dark but obvious <UNK> , with the <UNK> <UNK> can the <UNK> <UNK> fun as the <UNK> <UNK>\n",
            "Epoch: 135 Cost: 0.208495989 Sample: <S> in a moving <UNK> 'll do well as well you <UNK> from the <UNK> <UNK> by the film of the\n",
            "Epoch: 136 Cost: 0.208277836 Sample: <S> <UNK> , <UNK> yet <UNK> film , , but it not always <UNK> to its <UNK> love . </S> <PAD>\n",
            "Epoch: 137 Cost: 0.208573252 Sample: <S> gets the <UNK> piece of drama with horror comedy , simply a <UNK> <UNK> , and <UNK> <UNK> <UNK> ...\n",
            "Epoch: 138 Cost: 0.208420947 Sample: <S> this melodrama is a <UNK> <UNK> of <UNK> of its <UNK> seem <UNK> , <UNK> that it 's great ,\n",
            "Epoch: 139 Cost: 0.208300844 Sample: <S> despite <UNK> ... but the <UNK> is the <UNK> elements of <UNK> <UNK> themselves in that it 's a <UNK>\n",
            "Epoch: 140 Cost: 0.20832482 Sample: <S> <UNK> on a <UNK> <UNK> of the <UNK> sequel ' but not probably from the <UNK> <UNK> , but it\n",
            "Epoch: 141 Cost: 0.208221987 Sample: <S> in <UNK> , but the <UNK> , but of the movie <UNK> minutes of french <UNK> : to anyone occasionally\n",
            "Epoch: 142 Cost: 0.208170831 Sample: <S> for this <UNK> <UNK> , and in <UNK> <UNK> its <UNK> , <UNK> things , but it still is <UNK>\n",
            "Epoch: 143 Cost: 0.20823586 Sample: <S> <UNK> <UNK> that you <UNK> movies , is that a mostly <UNK> in the laughs <UNK> film of <UNK> of\n",
            "Epoch: 144 Cost: 0.20815748 Sample: <S> a <UNK> sequel and <UNK> a way <UNK> and then <UNK> line , he and <UNK> about the <UNK> life\n",
            "Epoch: 145 Cost: 0.208091766 Sample: <S> rich in and <UNK> love is something of <UNK> screen , and history <UNK> in this , <UNK> <UNK> to\n",
            "Epoch: 146 Cost: 0.207948014 Sample: <S> <UNK> <UNK> <UNK> <UNK> , as <UNK> off as it some like <UNK> of your <UNK> back and of a\n",
            "Epoch: 147 Cost: 0.208030239 Sample: <S> <UNK> does <UNK> in a movie <UNK> , <UNK> by <UNK> thing for most <UNK> of <UNK> 's <UNK> <UNK>\n",
            "Epoch: 148 Cost: 0.208003968 Sample: <S> <UNK> up here in <UNK> , <UNK> has made it 's flat - <UNK> them , a <UNK> and <UNK>\n",
            "Epoch: 149 Cost: 0.207878128 Sample: <S> some <UNK> of an <UNK> over on the movie over but <UNK> this is <UNK> has been a <UNK> <UNK>\n",
            "Epoch: 150 Cost: 0.207935169 Sample: <S> the <UNK> as one end , <UNK> sometimes , <UNK> and <UNK> to be the screenplay enough a perfect movie\n",
            "Epoch: 151 Cost: 0.207804143 Sample: <S> -- mr. <UNK> film in <UNK> so much acting to <UNK> from his movies , and part a plot <UNK>\n",
            "Epoch: 152 Cost: 0.207816869 Sample: <S> a <UNK> , it will has the <UNK> <UNK> did ... has a long sure to see it you have\n",
            "Epoch: 153 Cost: 0.207588598 Sample: <S> it <UNK> is the <UNK> <UNK> , and it is n't a one time and a too <UNK> in their\n",
            "Epoch: 154 Cost: 0.207727641 Sample: <S> -lrb- the <UNK> <UNK> that 's a <UNK> drama and <UNK> with quirky <UNK> as its surprisingly <UNK> family documentary\n",
            "Epoch: 155 Cost: 0.2076765 Sample: <S> <UNK> like a <UNK> <UNK> <UNK> and <UNK> <UNK> its <UNK> , and brilliant <UNK> comedy to <UNK> <UNK> with\n",
            "Epoch: 156 Cost: 0.207629636 Sample: <S> a movie <UNK> <UNK> is n't <UNK> <UNK> , <UNK> , it will be as a few <UNK> in getting\n",
            "Epoch: 157 Cost: 0.207569614 Sample: <S> boy <UNK> here of the <UNK> <UNK> who 's so small <UNK> <UNK> by <UNK> drama that <UNK> <UNK> for\n",
            "Epoch: 158 Cost: 0.207706943 Sample: <S> this overall <UNK> and no <UNK> <UNK> <UNK> and <UNK> before <UNK> to <UNK> to the film <UNK> on and\n",
            "Epoch: 159 Cost: 0.207743824 Sample: <S> an <UNK> and amusing , as when you <UNK> up to <UNK> his <UNK> yet the <UNK> sequences , and\n",
            "Epoch: 160 Cost: 0.207615614 Sample: <S> to a <UNK> <UNK> with <UNK> and energy , who <UNK> in <UNK> that 's point as a rare <UNK>\n",
            "Epoch: 161 Cost: 0.207438543 Sample: <S> a <UNK> quirky , and predictable <UNK> <UNK> is that is a -rrb- <UNK> <UNK> <UNK> here , and <UNK>\n",
            "Epoch: 162 Cost: 0.20757933 Sample: <S> it offers <UNK> beautifully <UNK> with a <UNK> <UNK> energy and there , you are like <UNK> . </S> <PAD>\n",
            "Epoch: 163 Cost: 0.207638249 Sample: <S> the <UNK> <UNK> us <UNK> <UNK> , but it because those <UNK> <UNK> but , it would <UNK> a rather\n",
            "Epoch: 164 Cost: 0.207470477 Sample: <S> not a <UNK> scenes , and even some of <UNK> <UNK> on the themselves and <UNK> <UNK> and <UNK> by\n",
            "Epoch: 165 Cost: 0.207346722 Sample: <S> plays like the great <UNK> narrative proves to <UNK> is great <UNK> <UNK> spirit and <UNK> , contrived or <UNK>\n",
            "Epoch: 166 Cost: 0.207387954 Sample: <S> you have as one <UNK> are , takes <UNK> with <UNK> a <UNK> that <UNK> the <UNK> <UNK> <UNK> <UNK>\n",
            "Epoch: 167 Cost: 0.207375377 Sample: <S> how through it <UNK> to social and <UNK> <UNK> <UNK> as with <UNK> seems to a <UNK> <UNK> summer 's\n",
            "Epoch: 168 Cost: 0.207430899 Sample: <S> a <UNK> by <UNK> the <UNK> <UNK> and more <UNK> and <UNK> is of its <UNK> , `` <UNK> ,\n",
            "Epoch: 169 Cost: 0.207390666 Sample: <S> <UNK> , if i <UNK> <UNK> on <UNK> , and <UNK> some <UNK> of <UNK> <UNK> <UNK> of <UNK> .\n",
            "Epoch: 170 Cost: 0.207285598 Sample: <S> <UNK> every <UNK> with comedy is <UNK> an slow , love , <UNK> -rrb- this <UNK> <UNK> each sense and\n",
            "Epoch: 171 Cost: 0.207324237 Sample: <S> i <UNK> <UNK> 's <UNK> <UNK> tale of this movie , <UNK> than <UNK> , the most romance star ending\n",
            "Epoch: 172 Cost: 0.207350984 Sample: <S> <UNK> , <UNK> , that it 's has not the modern <UNK> <UNK> that has its <UNK> works <UNK> .\n",
            "Epoch: 173 Cost: 0.207167685 Sample: <S> <UNK> <UNK> , this laughs by the sweet days <UNK> <UNK> and , <UNK> that you do n't <UNK> with\n",
            "Epoch: 174 Cost: 0.207181439 Sample: <S> it ... may does well a dramatic <UNK> <UNK> <UNK> , is it is , films <UNK> and even though\n",
            "Epoch: 175 Cost: 0.207118675 Sample: <S> if mr. <UNK> in you a <UNK> has a <UNK> the first cast , but you do in an <UNK>\n",
            "Epoch: 176 Cost: 0.207048193 Sample: <S> the same head men by <UNK> humor , and the <UNK> its <UNK> <UNK> <UNK> the <UNK> . </S> <PAD>\n",
            "Epoch: 177 Cost: 0.207079872 Sample: <S> it is -- as well <UNK> , to <UNK> and not very good <UNK> - , <UNK> <UNK> to a\n",
            "Epoch: 178 Cost: 0.207061887 Sample: <S> it with a <UNK> <UNK> enough to <UNK> <UNK> -- sweet and <UNK> does , and <UNK> <UNK> <UNK> ,\n",
            "Epoch: 179 Cost: 0.206942916 Sample: <S> <UNK> for <UNK> story than and is <UNK> there is <UNK> the <UNK> <UNK> of those most <UNK> in any\n",
            "Epoch: 180 Cost: 0.207155973 Sample: <S> when you if they end you <UNK> with , <UNK> here of ever all itself at his own <UNK> .\n",
            "Epoch: 181 Cost: 0.206899509 Sample: <S> boy <UNK> of <UNK> of the <UNK> and ultimately <UNK> <UNK> and you , the ` <UNK> <UNK> short of\n",
            "Epoch: 182 Cost: 0.20707868 Sample: <S> in its <UNK> <UNK> , especially and <UNK> <UNK> <UNK> <UNK> , especially is <UNK> enough of mostly <UNK> to\n",
            "Epoch: 183 Cost: 0.20683828 Sample: <S> far the reason to a <UNK> '' to be a <UNK> story does ! 's not a <UNK> <UNK> <UNK>\n",
            "Epoch: 184 Cost: 0.206974387 Sample: <S> ... ` <UNK> for its time <UNK> , by `` <UNK> <UNK> and of its <UNK> , directed and full\n",
            "Epoch: 185 Cost: 0.207025513 Sample: <S> makes it all : its <UNK> <UNK> , the hollywood ending is a <UNK> <UNK> the picture about director <UNK>\n",
            "Epoch: 186 Cost: 0.206931174 Sample: <S> if you , <UNK> something to <UNK> <UNK> <UNK> <UNK> <UNK> drama of this movie 's a wild sense and\n",
            "Epoch: 187 Cost: 0.206752151 Sample: <S> for the <UNK> of this <UNK> <UNK> <UNK> and <UNK> , the story and <UNK> laughs to turn by the\n",
            "Epoch: 188 Cost: 0.20668824 Sample: <S> <UNK> to <UNK> too <UNK> has a <UNK> or light on film , and <UNK> <UNK> is about some movie\n",
            "Epoch: 189 Cost: 0.206634611 Sample: <S> too funny in this , <UNK> <UNK> is in this is good , is more <UNK> , the effort scenes\n",
            "Epoch: 190 Cost: 0.206926078 Sample: <S> if you probably like the quirky <UNK> in <UNK> or story of <UNK> heart and going in tired ways in\n",
            "Epoch: 191 Cost: 0.206754357 Sample: <S> rarely a dull tragedy and fans that ` do <UNK> <UNK> to make the movies like a <UNK> <UNK> <UNK>\n",
            "Epoch: 192 Cost: 0.206567049 Sample: <S> i do n't <UNK> you see , this would rather otherwise <UNK> <UNK> , but <UNK> down by most of\n",
            "Epoch: 193 Cost: 0.206671029 Sample: <S> a <UNK> <UNK> 's film <UNK> with the performances , or a way <UNK> in the one is <UNK> not\n",
            "Epoch: 194 Cost: 0.206544518 Sample: <S> the full , but the <UNK> narrative <UNK> is funny and more <UNK> for the <UNK> <UNK> movie <UNK> to\n",
            "Epoch: 195 Cost: 0.206675649 Sample: <S> far from the film but it gets a <UNK> , <UNK> <UNK> humor and is that it finally have been\n",
            "Epoch: 196 Cost: 0.206653401 Sample: <S> <UNK> 's as both the <UNK> <UNK> that is <UNK> with people and <UNK> ? . </S> <PAD> <PAD> <PAD>\n",
            "Epoch: 197 Cost: 0.206744581 Sample: <S> the <UNK> <UNK> and <UNK> film , and she are powerful enough <UNK> <UNK> to <UNK> the <UNK> on <UNK>\n",
            "Epoch: 198 Cost: 0.206487089 Sample: <S> it <UNK> , and people keep you you 're off in a really <UNK> <UNK> the film at never `\n",
            "Epoch: 199 Cost: 0.206622124 Sample: <S> ... a `` <UNK> , and <UNK> dull again and <UNK> a great <UNK> of <UNK> , and bad that\n",
            "Epoch: 200 Cost: 0.206629425 Sample: <S> <UNK> might as <UNK> action tale year never into `` real first thing <UNK> , and the characters <UNK> him\n",
            "Epoch: 201 Cost: 0.206393048 Sample: <S> i was <UNK> filmmaking , in it that 's <UNK> and <UNK> <UNK> <UNK> of <UNK> <UNK> flick with the\n",
            "Epoch: 202 Cost: 0.20653528 Sample: <S> an interesting to keep us a <UNK> <UNK> a <UNK> <UNK> of its people and women may <UNK> <UNK> <UNK>\n",
            "Epoch: 203 Cost: 0.206543833 Sample: <S> a film <UNK> , and <UNK> or care about a <UNK> <UNK> <UNK> and rare <UNK> in the story ,\n",
            "Epoch: 204 Cost: 0.206557631 Sample: <S> <UNK> <UNK> more <UNK> of about a <UNK> this may find little <UNK> <UNK> on the <UNK> <UNK> filmmaker of\n",
            "Epoch: 205 Cost: 0.206326067 Sample: <S> i can <UNK> more and <UNK> as this movie is a <UNK> -- a <UNK> is probably more like a\n",
            "Epoch: 206 Cost: 0.206400245 Sample: <S> `` the <UNK> good <UNK> film of this <UNK> , '' without <UNK> , and a <UNK> love ... and\n",
            "Epoch: 207 Cost: 0.206426933 Sample: <S> ... but it is a <UNK> the movie <UNK> has to <UNK> <UNK> by the movie <UNK> take . </S>\n",
            "Epoch: 208 Cost: 0.206462562 Sample: <S> <UNK> about the <UNK> it is n't , the most <UNK> , of dark film that <UNK> , but he\n",
            "Epoch: 209 Cost: 0.206329033 Sample: <S> does instead works of the <UNK> to <UNK> <UNK> of a <UNK> a movie <UNK> look . </S> <PAD> <PAD>\n",
            "Epoch: 210 Cost: 0.206324086 Sample: <S> a moving matter of what the rare <UNK> <UNK> becomes <UNK> of the film its <UNK> be an <UNK> of\n",
            "Epoch: 211 Cost: 0.206320196 Sample: <S> it , <UNK> and the <UNK> in <UNK> that 's <UNK> 's also a <UNK> people who who makes <UNK>\n",
            "Epoch: 212 Cost: 0.206186831 Sample: <S> <UNK> and whether that he <UNK> <UNK> , when they you 're not what still is just . </S> <PAD>\n",
            "Epoch: 213 Cost: 0.206307679 Sample: <S> no ` <UNK> , <UNK> enough to <UNK> need to <UNK> on this film movie is also a <UNK> <UNK>\n",
            "Epoch: 214 Cost: 0.206094101 Sample: <S> one and <UNK> -- as it <UNK> <UNK> me has surprising -- <UNK> , part of <UNK> a <UNK> fans\n",
            "Epoch: 215 Cost: 0.206090778 Sample: <S> time or their spirit to <UNK> is nothing to call it ends , and one of <UNK> viewers of <UNK>\n",
            "Epoch: 216 Cost: 0.206074193 Sample: <S> <UNK> in to a <UNK> a <UNK> <UNK> <UNK> , and <UNK> <UNK> and of <UNK> by the <UNK> not\n",
            "Epoch: 217 Cost: 0.206058308 Sample: <S> with a good <UNK> wild camera and <UNK> series , as it has a really <UNK> <UNK> , <UNK> ,\n",
            "Epoch: 218 Cost: 0.206386447 Sample: <S> the heart full <UNK> script proves the dull <UNK> for a <UNK> <UNK> , with the original <UNK> <UNK> the\n",
            "Epoch: 219 Cost: 0.206163079 Sample: <S> no <UNK> than a `` <UNK> the <UNK> <UNK> <UNK> of what `` the <UNK> , <UNK> . </S> <PAD>\n",
            "Epoch: 220 Cost: 0.206110775 Sample: <S> <UNK> to death , easy to <UNK> the show <UNK> to <UNK> exercise in this <UNK> <UNK> <UNK> <UNK> is\n",
            "Epoch: 221 Cost: 0.206305265 Sample: <S> <UNK> slow for this <UNK> -lrb- of <UNK> <UNK> film of the death to <UNK> the director <UNK> away from\n",
            "Epoch: 222 Cost: 0.206102133 Sample: <S> the <UNK> plot and the <UNK> one of new <UNK> <UNK> and on its most part <UNK> death of .\n",
            "Epoch: 223 Cost: 0.206231892 Sample: <S> as the one <UNK> <UNK> , but <UNK> for an <UNK> than <UNK> gags are enough <UNK> to the <UNK>\n",
            "Epoch: 224 Cost: 0.205960959 Sample: <S> the <UNK> as a little <UNK> <UNK> <UNK> <UNK> and <UNK> has <UNK> are it a bit hard than `\n",
            "Epoch: 225 Cost: 0.205979705 Sample: <S> a <UNK> <UNK> of what they were that he has been a <UNK> <UNK> here on about a serious look\n",
            "Epoch: 226 Cost: 0.205968395 Sample: <S> <UNK> for humor and romantic documentary to <UNK> <UNK> a dark line and more fine movies else and two <UNK>\n",
            "Epoch: 227 Cost: 0.205942735 Sample: <S> <UNK> down by the an <UNK> of compelling to <UNK> <UNK> a too bad movie <UNK> and <UNK> of the\n",
            "Epoch: 228 Cost: 0.206040025 Sample: <S> despite an <UNK> <UNK> for the first movie <UNK> and <UNK> should have made <UNK> . </S> <PAD> <PAD> <PAD>\n",
            "Epoch: 229 Cost: 0.205873713 Sample: <S> <UNK> , and history and <UNK> of honest <UNK> is <UNK> out for the movie to <UNK> and that does\n",
            "Epoch: 230 Cost: 0.205920666 Sample: <S> though it a little <UNK> and <UNK> , it is she becomes a <UNK> <UNK> and as <UNK> characters ,\n",
            "Epoch: 231 Cost: 0.20586662 Sample: <S> <UNK> into the screenplay <UNK> <UNK> of the <UNK> it 's <UNK> a pleasure of place love , <UNK> and\n",
            "Epoch: 232 Cost: 0.205742255 Sample: <S> -lrb- <UNK> -lrb- of <UNK> and these <UNK> ; a <UNK> , and <UNK> and beautifully to by most often\n",
            "Epoch: 233 Cost: 0.20583491 Sample: <S> <UNK> 's <UNK> man 's performance there 's probably worth a <UNK> heart and the <UNK> <UNK> as <UNK> storytelling\n",
            "Epoch: 234 Cost: 0.205751657 Sample: <S> a <UNK> <UNK> a <UNK> `` life it with each <UNK> for how often yet -- <UNK> sequences . </S>\n",
            "Epoch: 235 Cost: 0.206055298 Sample: <S> <UNK> <UNK> , but it feels too movie is <UNK> a movie <UNK> to give he <UNK> <UNK> that ?\n",
            "Epoch: 236 Cost: 0.205779135 Sample: <S> it <UNK> <UNK> an <UNK> <UNK> and his personal <UNK> feature by a man <UNK> , <UNK> and <UNK> as\n",
            "Epoch: 237 Cost: 0.205849692 Sample: <S> you 'll <UNK> <UNK> , from in the <UNK> <UNK> <UNK> -- an <UNK> than <UNK> <UNK> 's two <UNK>\n",
            "Epoch: 238 Cost: 0.205694452 Sample: <S> one at most visual `` <UNK> <UNK> 's has -- <UNK> a thoughtful life and <UNK> at , <UNK> <UNK>\n",
            "Epoch: 239 Cost: 0.205664948 Sample: <S> it in which a <UNK> to <UNK> to the <UNK> and <UNK> <UNK> , and <UNK> to <UNK> <UNK> .\n",
            "Epoch: 240 Cost: 0.205893859 Sample: <S> <UNK> <UNK> and obvious in <UNK> , which makes her feels enough to believe that the story and <UNK> is\n",
            "Epoch: 241 Cost: 0.205716267 Sample: <S> a <UNK> <UNK> '' by , <UNK> is <UNK> film of the <UNK> <UNK> of <UNK> <UNK> in its <UNK>\n",
            "Epoch: 242 Cost: 0.205567971 Sample: <S> so <UNK> <UNK> documentary : <UNK> , as it is <UNK> one of <UNK> , and it is - you\n",
            "Epoch: 243 Cost: 0.205768779 Sample: <S> a <UNK> drama of such a <UNK> <UNK> <UNK> -- without <UNK> , <UNK> <UNK> than for some <UNK> of\n",
            "Epoch: 244 Cost: 0.205699101 Sample: <S> that ... without the <UNK> well <UNK> about a opera <UNK> will <UNK> has nothing not to the <UNK> also\n",
            "Epoch: 245 Cost: 0.205711469 Sample: <S> that is <UNK> and i still found in a <UNK> to , <UNK> and another young see it on and\n",
            "Epoch: 246 Cost: 0.205654472 Sample: <S> occasionally fascinating <UNK> 's the movie over as <UNK> to that <UNK> with a lack <UNK> of its <UNK> out\n",
            "Epoch: 247 Cost: 0.205539808 Sample: <S> i have <UNK> for it is so <UNK> film is in a <UNK> and going study to your mind <UNK>\n",
            "Epoch: 248 Cost: 0.205571771 Sample: <S> one of a <UNK> <UNK> but in <UNK> <UNK> that will <UNK> new <UNK> film and women in a <UNK>\n",
            "Epoch: 249 Cost: 0.20558317 Sample: <S> despite <UNK> on the <UNK> make . none of his <UNK> movie that is both <UNK> without the movie ,\n",
            "Epoch: 250 Cost: 0.205662102 Sample: <S> this a wild sometimes <UNK> <UNK> that does make it , it i <UNK> will get a <UNK> . </S>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kPUX6OrgUva"
      },
      "source": [
        "Now we can draw as many samples as we like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94XdnKR5gUvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2be6f776-589b-4e3b-b9c3-e9f84f0b2604"
      },
      "source": [
        "for i in range(50): print(i+1, model.sample())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 <S> <UNK> it is not only to be <UNK> violence , but with the <UNK> work , in the <UNK> 's\n",
            "2 <S> a <UNK> ... by <UNK> <UNK> <UNK> <UNK> , different from <UNK> images , <UNK> than like about acting as\n",
            "3 <S> though <UNK> , with which predictable <UNK> on its <UNK> <UNK> <UNK> into `` the <UNK> may sense <UNK> <UNK>\n",
            "4 <S> <UNK> <UNK> elements of comes <UNK> into the <UNK> characters in a <UNK> this on <UNK> story that <UNK> the\n",
            "5 <S> ` the big art tale , that offers a <UNK> , <UNK> a <UNK> so much interesting with <UNK> <UNK>\n",
            "6 <S> a <UNK> at his place , mostly <UNK> the performances , the way , <UNK> intelligence and <UNK> <UNK> to\n",
            "7 <S> if it still not love the <UNK> seem more <UNK> into to like a <UNK> <UNK> and a lot better\n",
            "8 <S> the reason many without the star <UNK> <UNK> <UNK> <UNK> to your <UNK> some of <UNK> <UNK> <UNK> . </S>\n",
            "9 <S> visually <UNK> satisfying <UNK> -rrb- , in <UNK> comedy , <UNK> of the first movie <UNK> a brilliant boy ,\n",
            "10 <S> dumb people , <UNK> <UNK> culture from the <UNK> <UNK> that 's <UNK> <UNK> by the energy , an action\n",
            "11 <S> a <UNK> the intelligent <UNK> film ... he , is a <UNK> <UNK> of the <UNK> <UNK> <UNK> to <UNK>\n",
            "12 <S> <UNK> and thoughtful , in a <UNK> a <UNK> the <UNK> <UNK> its <UNK> <UNK> and <UNK> the surprisingly <UNK>\n",
            "13 <S> <UNK> an <UNK> -rrb- flick , <UNK> <UNK> remains , ultimately that <UNK> , in the charm that even perfectly\n",
            "14 <S> its performances by <UNK> its sense and <UNK> movies of the <UNK> <UNK> ` <UNK> <UNK> , <UNK> is <UNK>\n",
            "15 <S> <UNK> to <UNK> by in the <UNK> <UNK> has <UNK> right , with his <UNK> with which `` <UNK> never\n",
            "16 <S> overall a <UNK> . that you 're very fun so boring and that <UNK> , the <UNK> to do it\n",
            "17 <S> <UNK> to the <UNK> <UNK> work , does seems as <UNK> <UNK> , for <UNK> to <UNK> over , the\n",
            "18 <S> there are <UNK> worth <UNK> <UNK> than a way <UNK> of the mood cinematic story <UNK> and <UNK> the <UNK>\n",
            "19 <S> what -lrb- this that <UNK> <UNK> and the <UNK> <UNK> <UNK> is <UNK> <UNK> <UNK> to like than the <UNK>\n",
            "20 <S> for <UNK> their guys <UNK> from <UNK> are much <UNK> of an <UNK> <UNK> lives of <UNK> <UNK> and <UNK>\n",
            "21 <S> a <UNK> <UNK> ... any movie about <UNK> <UNK> to their own <UNK> the <UNK> , <UNK> of soap ,\n",
            "22 <S> yet it 's <UNK> , a <UNK> <UNK> of <UNK> <UNK> so <UNK> , you <UNK> and <UNK> if <UNK>\n",
            "23 <S> <UNK> <UNK> <UNK> like on and the long <UNK> and predictable , and <UNK> from its <UNK> <UNK> to <UNK>\n",
            "24 <S> what makes you feel <UNK> to have a lot like a solid <UNK> <UNK> or visual <UNK> those <UNK> of\n",
            "25 <S> <UNK> ! in the <UNK> along , <UNK> to <UNK> the <UNK> <UNK> 's <UNK> <UNK> to a <UNK> .\n",
            "26 <S> <UNK> , but <UNK> with my big <UNK> <UNK> to make the <UNK> <UNK> <UNK> little of them , though\n",
            "27 <S> horror movies than that there you , <UNK> has comic <UNK> , at <UNK> <UNK> and , though in the\n",
            "28 <S> <UNK> but is that they <UNK> <UNK> , only for a wild <UNK> <UNK> of a bad <UNK> . </S>\n",
            "29 <S> it right <UNK> <UNK> <UNK> , <UNK> <UNK> , <UNK> and the <UNK> <UNK> your <UNK> ... <UNK> was <UNK>\n",
            "30 <S> the director <UNK> movie he has <UNK> never <UNK> <UNK> <UNK> in a <UNK> <UNK> study , film about as\n",
            "31 <S> only a <UNK> after a <UNK> <UNK> or the <UNK> <UNK> and <UNK> <UNK> script , the <UNK> comedy as\n",
            "32 <S> a <UNK> their <UNK> , <UNK> <UNK> by a <UNK> <UNK> <UNK> performances of the <UNK> <UNK> and political <UNK>\n",
            "33 <S> this <UNK> <UNK> and the <UNK> <UNK> ` <UNK> , <UNK> <UNK> <UNK> that the <UNK> and surprisingly well as\n",
            "34 <S> the most complex <UNK> 's just very short <UNK> and it down by a <UNK> powerful first film <UNK> ,\n",
            "35 <S> a beautiful performances <UNK> on with an interesting than <UNK> is <UNK> just as in any if <UNK> boy of\n",
            "36 <S> <UNK> <UNK> its guys sometimes has no as <UNK> real work , nothing as i <UNK> <UNK> <UNK> for a\n",
            "37 <S> <UNK> in a great <UNK> entertainment as much like as it <UNK> 's sense of <UNK> <UNK> , <UNK> of\n",
            "38 <S> <UNK> and the life <UNK> with <UNK> -rrb- better than despite <UNK> director <UNK> can <UNK> , with <UNK> say\n",
            "39 <S> just a many fairly <UNK> the great <UNK> <UNK> , <UNK> : the movie , the full <UNK> girl ,\n",
            "40 <S> as ` <UNK> , and <UNK> that we have <UNK> <UNK> whose <UNK> us from the two <UNK> actors <UNK>\n",
            "41 <S> ... -lrb- honest funny visual <UNK> that <UNK> , delivers as its <UNK> formula as <UNK> <UNK> day . </S>\n",
            "42 <S> -lrb- like <UNK> into an <UNK> <UNK> with <UNK> and of <UNK> <UNK> , one of one of <UNK> <UNK>\n",
            "43 <S> a surprising <UNK> <UNK> of the <UNK> <UNK> <UNK> <UNK> and his <UNK> , <UNK> kids '' comic <UNK> ,\n",
            "44 <S> <UNK> an <UNK> you <UNK> <UNK> 's <UNK> <UNK> , <UNK> should be what , a few <UNK> <UNK> <UNK>\n",
            "45 <S> <UNK> and <UNK> <UNK> , lack of <UNK> <UNK> is both as in the <UNK> world <UNK> to you <UNK>\n",
            "46 <S> <UNK> plot the <UNK> , a <UNK> performances , could <UNK> <UNK> <UNK> 's attempt to come in long <UNK>\n",
            "47 <S> it is one is in <UNK> set the full of <UNK> than the whole <UNK> <UNK> story and <UNK> <UNK>\n",
            "48 <S> the <UNK> to <UNK> the rather scenes like a <UNK> on the <UNK> <UNK> nothing will do that own <UNK>\n",
            "49 <S> most <UNK> <UNK> , <UNK> on the <UNK> <UNK> the <UNK> it live up what the <UNK> , <UNK> that\n",
            "50 <S> takes a <UNK> <UNK> does all watching its <UNK> <UNK> work , <UNK> but its <UNK> of humor or <UNK>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HsJ9ONQgUvk"
      },
      "source": [
        "### Part 2: Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pF05DtcWgUvm"
      },
      "source": [
        "**Question 1:** Looking at the samples that your model produced towards the end of training, point out three properties of (written) English that it seems to have learned."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The three most outstanding properties I would like to emphasize are:\n",
        "+ The high percentage of correct use of prepositions.\n",
        "+ The structure noun + adjective shown in the model is accurate.\n",
        "+ Punctuation reveals good results. Commas, dots and the 's apostrophes are adequately placed, and the use of colon (:) is appropriate - as we can detect it before a list, for instance.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enrHI-5UhqYd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MeNJ9k-gUvp"
      },
      "source": [
        "**Question 2:** If we could make the model as big as we wanted, train as long as we wanted, and adjust or remove dropout at will, could we ever get the model to reach a cost value of 0.0? In a single sentence, say why."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ We could fulfill that expectation given the terms exposed in the question. Nevertheless, the exploitation of resources and the risk of committing overfitting could be too high."
      ],
      "metadata": {
        "id": "JCNA3AkLhq5T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXKts8jJgUvs"
      },
      "source": [
        "**Question 3:** Give an example of a situation where the LSTM language model's ability to propagate information across many steps (when trained for long enough, at least) would cause it to reach a better cost value than a model like a simple RNN without that ability. (Answer in one sentence or so.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ RNN are recognized due to their short memory (for instance, RNN forget easily a topic in a long text generated automatically), meanwhile LSTM maintain the plot of a text longer. Therefore, LSTM will perform better."
      ],
      "metadata": {
        "id": "Qpjz6pCVhroJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75iq_DjIgUvv"
      },
      "source": [
        "**Question 4:** Would the model be any worse if we were to just delete unknown words instead of using an `<UNK>` token? (Answer in one sentence or so.)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "+ It would be worse, because if we don't highlight to the model what are the missing elements of a text, it could learn in a way that it seems that text, with words deleted, are naturally present in that way in natural language."
      ],
      "metadata": {
        "id": "Oq6YH8RFhsNh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb-PCtUagUvy"
      },
      "source": [
        "# Atribution:\n",
        "Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"
      ]
    }
  ]
}