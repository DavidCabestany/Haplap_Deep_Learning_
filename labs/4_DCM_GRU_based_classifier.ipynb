{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MJIR4_GRU_based_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPFakP2qNnpx"
      },
      "source": [
        "# Lab4: Sentiment with GRUs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnmzS4xcNnp2"
      },
      "source": [
        "In this assignment, you'll convert the RNN sentiment classifier from last time into a **GRU** RNN sentiment classifier. While the small dataset and tiny vocabulary that we're using here (for speed) will limit the performance of the model, it should still do substantially better than the plain RNN.\n",
        "\n",
        "![](http://vignette1.wikia.nocookie.net/despicableme/images/b/ba/Gru.jpg/revision/latest/scale-to-width-down/250?cb=20130711023954)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIO-J9F4Nnp5"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfdccRPyNnp-"
      },
      "source": [
        "First, let's load the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKXUkp6lOTxp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d86c2aa6-1710-44dd-a52a-a1292e011834"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFYDv62UNnqA"
      },
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# Let's do 2-way positive/negative classification instead of 5-way\n",
        "def load_sst_data(path,\n",
        "                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            example = {}\n",
        "            example['label'] = easy_label_map[int(line[1])]\n",
        "            if example['label'] is None:\n",
        "                continue\n",
        "            \n",
        "            # Strip out the parse information and the phrase labels--\n",
        "            # ---we don't need those here\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "    \n",
        "    random.seed(1)\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "   \n",
        "sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n",
        "training_set = load_sst_data(sst_home + '/train.txt')\n",
        "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
        "test_set = load_sst_data(sst_home + '/test.txt')\n",
        "\n",
        "# Note: Unlike with k-nearest neighbors, evaluation here should be fast, and we don't need to\n",
        "# trim down the dev and test sets. "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeiVYJONNnqK"
      },
      "source": [
        "Next, we'll convert the data to index vectors.\n",
        "\n",
        "To simplify your implementation, we'll use a fixed unrolling length of 20. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHhrsUHGNnqM"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "def sentence_to_padded_index_sequence(datasets):\n",
        "    '''Annotates datasets with feature vectors.'''\n",
        "    \n",
        "\n",
        "    PADDING = \"<PAD>\"\n",
        "    UNKNOWN = \"<UNK>\"\n",
        "    SEQ_LEN = 20\n",
        "    \n",
        "    # Extract vocabulary\n",
        "    def tokenize(string):\n",
        "        return string.lower().split()\n",
        "    \n",
        "    word_counter = collections.Counter()\n",
        "    for example in datasets[0]:\n",
        "        word_counter.update(tokenize(example['text']))\n",
        "    \n",
        "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
        "    vocabulary = list(vocabulary)\n",
        "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
        "        \n",
        "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
        "        \n",
        "    for i, dataset in enumerate(datasets):\n",
        "        for example in dataset:\n",
        "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
        "            \n",
        "            token_sequence = tokenize(example['text'])\n",
        "            padding = SEQ_LEN - len(token_sequence)\n",
        "            \n",
        "            for i in range(SEQ_LEN):\n",
        "                if i >= padding:\n",
        "                    if token_sequence[i - padding] in word_indices:\n",
        "                        index = word_indices[token_sequence[i - padding]]\n",
        "                    else:\n",
        "                        index = word_indices[UNKNOWN]\n",
        "                else:\n",
        "                    index = word_indices[PADDING]\n",
        "                example['index_sequence'][i] = index\n",
        "    return indices_to_words, word_indices\n",
        "    \n",
        "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FrcDlK8NnqU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09243682-ac26-4709-b97c-750dec47b141"
      },
      "source": [
        "print(training_set[18])\n",
        "print(len(word_indices))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 1, 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([   0,    0,    0,    0,    0,    0,    0,    0,    0,  505, 1000,\n",
            "          1,    1,  676,    1,    1, 1203,    1,   95,   79], dtype=int32)}\n",
            "1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEKDVRYwNnqd"
      },
      "source": [
        "def evaluate_classifier(classifier, eval_set):\n",
        "    correct = 0\n",
        "    hypotheses = classifier(eval_set)\n",
        "    for i, example in enumerate(eval_set):\n",
        "        hypothesis = hypotheses[i]\n",
        "        if hypothesis == example['label']:\n",
        "            correct += 1        \n",
        "    return correct / float(len(eval_set))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zBLwIVNhNnqj"
      },
      "source": [
        "## Assignments: Building the RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_q4prT9Nnqn"
      },
      "source": [
        "The class below is a solved version of last week's RNN exercise. The only change I've made is to increase the learning rate, since GRUs are less likely to do crazy things during optimization. Your job is to convert it into a GRU model. You should have to:\n",
        "\n",
        "- **TODO1**: Add additional trained parameters.\n",
        "- **TODO2**: Modify the `step()` function.\n",
        "- **TODO3**: Modify L2 regularization to incorporate the new parameters.\n",
        "\n",
        "You shouldn't have to edit anything outside of `__init__()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GXbwUBINnqr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "abb851cc-94d1-4d58-8472-66f9263807a6"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow==2.4.0\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.4.0\n",
            "  Downloading tensorflow-2.4.0-cp37-cp37m-manylinux2010_x86_64.whl (394.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 394.7 MB 17 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.19.5)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.3.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.15.0)\n",
            "Collecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting tensorflow-estimator<2.5.0,>=2.4.0rc0\n",
            "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 64.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.6.3)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.37.1)\n",
            "Collecting grpcio~=1.32.0\n",
            "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 58.4 MB/s \n",
            "\u001b[?25hCollecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.12.0)\n",
            "Collecting h5py~=2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 53.9 MB/s \n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "Requirement already satisfied: tensorboard~=2.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (2.7.0)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.4.0) (3.17.3)\n",
            "Collecting gast==0.3.3\n",
            "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.4->tensorflow==2.4.0) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (4.10.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.4->tensorflow==2.4.0) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.4->tensorflow==2.4.0) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow==2.4.0) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow==2.4.0) (3.1.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=68723 sha256=7dadd4af7596423fd814c428263a63da86ec3dafecd6c39f5460f816ad97481b\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/76/4c/aa25851149f3f6d9785f6c869387ad82b3fd37582fa8147ac6\n",
            "Successfully built wrapt\n",
            "Installing collected packages: typing-extensions, grpcio, wrapt, tensorflow-estimator, h5py, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.10.0.2\n",
            "    Uninstalling typing-extensions-3.10.0.2:\n",
            "      Successfully uninstalled typing-extensions-3.10.0.2\n",
            "  Attempting uninstall: grpcio\n",
            "    Found existing installation: grpcio 1.43.0\n",
            "    Uninstalling grpcio-1.43.0:\n",
            "      Successfully uninstalled grpcio-1.43.0\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.13.3\n",
            "    Uninstalling wrapt-1.13.3:\n",
            "      Successfully uninstalled wrapt-1.13.3\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.7.0\n",
            "    Uninstalling tensorflow-estimator-2.7.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.7.0\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.7.0\n",
            "    Uninstalling tensorflow-2.7.0:\n",
            "      Successfully uninstalled tensorflow-2.7.0\n",
            "Successfully installed flatbuffers-1.12 gast-0.3.3 grpcio-1.32.0 h5py-2.10.0 tensorflow-2.4.0 tensorflow-estimator-2.4.0 typing-extensions-3.7.4.3 wrapt-1.12.1\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCzr4whzNnqy"
      },
      "source": [
        "class RNNSentimentClassifier:\n",
        "    def __init__(self, vocab_size, sequence_length):\n",
        "        # Define the hyperparameters\n",
        "        self.learning_rate = 1.0  # Should be about right\n",
        "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
        "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
        "        self.dim = 12  # The dimension of the hidden state of the RNN\n",
        "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
        "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
        "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
        "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
        "        self.l2_lambda = 0.001\n",
        "        \n",
        "        self.trainable_variables = []\n",
        "\n",
        "        # Define the parameters\n",
        "        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.E)\n",
        "\n",
        "        self.W_rnn = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_rnn = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_rnn)\n",
        "        self.trainable_variables.append(self.b_rnn)\n",
        "        \n",
        "        self.W_cl = tf.Variable(tf.random.normal([self.dim, 2], stddev=0.1))\n",
        "        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_cl)\n",
        "        self.trainable_variables.append(self.b_cl)\n",
        "        \n",
        "        # TODO1: Add additional GRU parameters\n",
        "        #input firs y lo multiplicamos por los parámetros\n",
        "               \n",
        "        self.W_z = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_z = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_z)\n",
        "        self.trainable_variables.append(self.b_z)\n",
        "\n",
        "        self.W_t = tf.Variable(tf.random.normal([self.embedding_dim + self.dim, self.dim], stddev=0.1))\n",
        "        self.b_t = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_t)\n",
        "        self.trainable_variables.append(self.b_t)\n",
        "\n",
        "                      \n",
        "    def model(self,x):\n",
        "        # Split up the inputs into individual tensors\n",
        "        self.x_slices = tf.split(x, self.sequence_length, 1)\n",
        "        self.h_zero = tf.zeros([self.batch_size, self.dim])\n",
        "\n",
        "        # Define one step of the RNN\n",
        "\n",
        "        # TODO2: Modify the step() function to compute GRU step\n",
        "        def step(x, h_prev):\n",
        "            emb = tf.nn.embedding_lookup(params=self.E,ids=x)\n",
        "            emb_h_prev = tf.concat([emb, h_prev], 1)\n",
        "            h = tf.nn.tanh(tf.matmul(emb_h_prev, self.W_rnn)  + self.b_rnn)\n",
        "            z_t = tf.math.sigmoid(tf.matmul(emb_h_prev, self.W_z)+ self.b_z)\n",
        "            r_t =  tf.math.sigmoid(tf.matmul(emb_h_prev, self.W_t)+ self.b_t)\n",
        "\n",
        "            h_t_ = tf.nn.tanh(tf.matmul(tf.concat([tf.math.multiply(r_t,h_prev), emb],axis=1),  self.W_rnn)+ self.b_rnn ) \n",
        "            h_t = tf.math.multiply((1- z_t),h_prev) + tf.math.multiply(z_t, h_t_)\n",
        "\n",
        "            return h_t\n",
        "                \n",
        "        h_prev = self.h_zero\n",
        "        \n",
        "        # Unroll the RNN\n",
        "        for t in range(self.sequence_length):\n",
        "            x_t = tf.reshape(self.x_slices[t], [-1])\n",
        "            h_prev = step(x_t, h_prev)\n",
        "        \n",
        "        # Compute the logits using one last linear layer\n",
        "        logits = tf.matmul(h_prev, self.W_cl) + self.b_cl\n",
        "        return logits\n",
        "        \n",
        "    def train(self, training_data, dev_set):\n",
        "        def get_minibatch(dataset, start_index, end_index):\n",
        "            indices = range(start_index, end_index)\n",
        "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
        "            labels = [dataset[i]['label'] for i in indices]\n",
        "            return vectors, labels\n",
        "    \n",
        "        print ('Training.')\n",
        "\n",
        "        # Training cycle\n",
        "        train_acc = []\n",
        "        dev_acc = []\n",
        "        epochs = []\n",
        "        for epoch in range(self.training_epochs):\n",
        "            random.shuffle(training_set)\n",
        "            avg_cost = 0.\n",
        "            total_batch = int(len(training_set) / self.batch_size)\n",
        "            \n",
        "            # Loop over all batches in epoch\n",
        "            for i in range(total_batch):\n",
        "                # Assemble a minibatch of the next B examples\n",
        "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
        "                                                                    self.batch_size * i, \n",
        "                                                                    self.batch_size * (i + 1))\n",
        "\n",
        "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
        "                # cost function for logging\n",
        "                with tf.GradientTape() as tape:\n",
        "                  logits = self.model(minibatch_vectors)\n",
        "                  # Define the L2 cost\n",
        "                  # TODO3: Modify L2 regularization to incorporate the new parameters.\n",
        "                  l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_rnn)) +\n",
        "                                                   tf.reduce_sum(tf.square(self.W_cl))+\n",
        "                                                   tf.reduce_sum(tf.square(self.W_t)) +\n",
        "                                                   tf.reduce_sum(tf.square(self.W_z)))\n",
        "        \n",
        "                  # Define the cost function (here, the softmax exp and sum are built in)\n",
        "                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + l2_cost)\n",
        "        \n",
        "                # This  performs the main SGD update equation with gradient clipping\n",
        "                optimizer = tf.optimizers.SGD(self.learning_rate)\n",
        "                gradients = tape.gradient(total_cost, self.trainable_variables)\n",
        "                gvs = zip(gradients, self.trainable_variables)\n",
        "                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
        "                optimizer.apply_gradients(capped_gvs)\n",
        "                                  \n",
        "                # Compute average loss\n",
        "                avg_cost += total_cost / total_batch\n",
        "                \n",
        "            # Display some statistics about the step\n",
        "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
        "            if (epoch+1) % self.display_epoch_freq == 0:\n",
        "                dev_acc.append(evaluate_classifier(self.classify, dev_set[0:256]))\n",
        "                train_acc.append(evaluate_classifier(self.classify, training_set[0:256]))\n",
        "                epochs.append(epoch+1)\n",
        "                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
        "                    \"Dev acc:\", dev_acc[-1], \\\n",
        "                    \"Train acc:\", train_acc[-1])  \n",
        "        return train_acc, dev_acc, epochs\n",
        "    \n",
        "    def classify(self, examples):\n",
        "        # This classifies a list of examples\n",
        "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
        "        logits = self.model(vectors)\n",
        "        return np.argmax(logits, axis=1)\n",
        "\n",
        "np.random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "classifier = RNNSentimentClassifier(len(word_indices), 20)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0ZpROqlNnq6"
      },
      "source": [
        "Now let's train it. If the GRU is doing what it should, you should reach 80% accuracy within your first 200 epochs—a substantial improvement over the 70% figure we saw last week."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlBwvZT9Nnq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0e3dd4-f4bc-446a-ac91-c8e09b1d9e3c"
      },
      "source": [
        "train_acc, dev_acc, epochs = classifier.train(training_set, dev_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training.\n",
            "Epoch: 5 Cost: 0.697260499 Dev acc: 0.5546875 Train acc: 0.546875\n",
            "Epoch: 10 Cost: 0.695174217 Dev acc: 0.4453125 Train acc: 0.4453125\n",
            "Epoch: 15 Cost: 0.694430172 Dev acc: 0.5546875 Train acc: 0.484375\n",
            "Epoch: 20 Cost: 0.693571448 Dev acc: 0.5546875 Train acc: 0.546875\n",
            "Epoch: 25 Cost: 0.693248928 Dev acc: 0.5546875 Train acc: 0.50390625\n",
            "Epoch: 30 Cost: 0.693099082 Dev acc: 0.4453125 Train acc: 0.42578125\n",
            "Epoch: 35 Cost: 0.693525 Dev acc: 0.5546875 Train acc: 0.58203125\n",
            "Epoch: 40 Cost: 0.693741381 Dev acc: 0.5546875 Train acc: 0.546875\n",
            "Epoch: 45 Cost: 0.693315148 Dev acc: 0.5546875 Train acc: 0.515625\n",
            "Epoch: 50 Cost: 0.692951381 Dev acc: 0.5546875 Train acc: 0.5546875\n",
            "Epoch: 55 Cost: 0.692522824 Dev acc: 0.5546875 Train acc: 0.5\n",
            "Epoch: 60 Cost: 0.69379133 Dev acc: 0.5546875 Train acc: 0.484375\n",
            "Epoch: 65 Cost: 0.693052113 Dev acc: 0.5546875 Train acc: 0.51953125\n",
            "Epoch: 70 Cost: 0.692752123 Dev acc: 0.4453125 Train acc: 0.50390625\n",
            "Epoch: 75 Cost: 0.69272548 Dev acc: 0.5546875 Train acc: 0.50390625\n",
            "Epoch: 80 Cost: 0.692847967 Dev acc: 0.5546875 Train acc: 0.51171875\n",
            "Epoch: 85 Cost: 0.692870915 Dev acc: 0.5546875 Train acc: 0.6015625\n",
            "Epoch: 90 Cost: 0.693078935 Dev acc: 0.5546875 Train acc: 0.53125\n",
            "Epoch: 95 Cost: 0.69260937 Dev acc: 0.5546875 Train acc: 0.546875\n",
            "Epoch: 100 Cost: 0.693228304 Dev acc: 0.5546875 Train acc: 0.53515625\n",
            "Epoch: 105 Cost: 0.692525 Dev acc: 0.5546875 Train acc: 0.5078125\n",
            "Epoch: 110 Cost: 0.693358421 Dev acc: 0.5703125 Train acc: 0.578125\n",
            "Epoch: 115 Cost: 0.69288075 Dev acc: 0.546875 Train acc: 0.53125\n",
            "Epoch: 120 Cost: 0.692243457 Dev acc: 0.56640625 Train acc: 0.51171875\n",
            "Epoch: 125 Cost: 0.691378474 Dev acc: 0.56640625 Train acc: 0.59765625\n",
            "Epoch: 130 Cost: 0.689947963 Dev acc: 0.5546875 Train acc: 0.4921875\n",
            "Epoch: 135 Cost: 0.689091802 Dev acc: 0.55859375 Train acc: 0.51953125\n",
            "Epoch: 140 Cost: 0.690615594 Dev acc: 0.57421875 Train acc: 0.55859375\n",
            "Epoch: 145 Cost: 0.686076283 Dev acc: 0.609375 Train acc: 0.59375\n",
            "Epoch: 150 Cost: 0.674711525 Dev acc: 0.640625 Train acc: 0.63671875\n",
            "Epoch: 155 Cost: 0.673287928 Dev acc: 0.6953125 Train acc: 0.65234375\n",
            "Epoch: 160 Cost: 0.65473628 Dev acc: 0.546875 Train acc: 0.609375\n",
            "Epoch: 165 Cost: 0.619172573 Dev acc: 0.69921875 Train acc: 0.6796875\n",
            "Epoch: 170 Cost: 0.581349909 Dev acc: 0.71484375 Train acc: 0.73046875\n",
            "Epoch: 175 Cost: 0.548635721 Dev acc: 0.71875 Train acc: 0.7421875\n",
            "Epoch: 180 Cost: 0.527749062 Dev acc: 0.765625 Train acc: 0.80078125\n",
            "Epoch: 185 Cost: 0.502794445 Dev acc: 0.78515625 Train acc: 0.78125\n",
            "Epoch: 190 Cost: 0.48723349 Dev acc: 0.7890625 Train acc: 0.828125\n",
            "Epoch: 195 Cost: 0.475968301 Dev acc: 0.80078125 Train acc: 0.8046875\n",
            "Epoch: 200 Cost: 0.46893236 Dev acc: 0.80859375 Train acc: 0.77734375\n",
            "Epoch: 205 Cost: 0.44035244 Dev acc: 0.8046875 Train acc: 0.79296875\n",
            "Epoch: 210 Cost: 0.426241815 Dev acc: 0.7890625 Train acc: 0.83203125\n",
            "Epoch: 215 Cost: 0.422676712 Dev acc: 0.8125 Train acc: 0.8671875\n",
            "Epoch: 220 Cost: 0.400614649 Dev acc: 0.8125 Train acc: 0.80859375\n",
            "Epoch: 225 Cost: 0.393322885 Dev acc: 0.80078125 Train acc: 0.796875\n",
            "Epoch: 230 Cost: 0.389513522 Dev acc: 0.8125 Train acc: 0.87109375\n",
            "Epoch: 235 Cost: 0.380353779 Dev acc: 0.80078125 Train acc: 0.87109375\n",
            "Epoch: 240 Cost: 0.357567102 Dev acc: 0.80078125 Train acc: 0.87109375\n",
            "Epoch: 245 Cost: 0.34094 Dev acc: 0.78125 Train acc: 0.875\n",
            "Epoch: 250 Cost: 0.344275862 Dev acc: 0.796875 Train acc: 0.9375\n",
            "Epoch: 255 Cost: 0.344713032 Dev acc: 0.7890625 Train acc: 0.8828125\n",
            "Epoch: 260 Cost: 0.317109913 Dev acc: 0.7734375 Train acc: 0.953125\n",
            "Epoch: 265 Cost: 0.332329094 Dev acc: 0.80859375 Train acc: 0.90234375\n",
            "Epoch: 270 Cost: 0.293459505 Dev acc: 0.79296875 Train acc: 0.8984375\n",
            "Epoch: 275 Cost: 0.327217162 Dev acc: 0.79296875 Train acc: 0.9140625\n",
            "Epoch: 280 Cost: 0.295581788 Dev acc: 0.7265625 Train acc: 0.81640625\n",
            "Epoch: 285 Cost: 0.270956486 Dev acc: 0.74609375 Train acc: 0.87109375\n",
            "Epoch: 290 Cost: 0.304999053 Dev acc: 0.78515625 Train acc: 0.96484375\n",
            "Epoch: 295 Cost: 0.266356736 Dev acc: 0.79296875 Train acc: 0.9375\n",
            "Epoch: 300 Cost: 0.253330648 Dev acc: 0.7890625 Train acc: 0.92578125\n",
            "Epoch: 305 Cost: 0.305028737 Dev acc: 0.80859375 Train acc: 0.92578125\n",
            "Epoch: 310 Cost: 0.276773363 Dev acc: 0.7421875 Train acc: 0.75390625\n",
            "Epoch: 315 Cost: 0.276930124 Dev acc: 0.77734375 Train acc: 0.9375\n",
            "Epoch: 320 Cost: 0.272948325 Dev acc: 0.765625 Train acc: 0.95703125\n",
            "Epoch: 325 Cost: 0.256058872 Dev acc: 0.78125 Train acc: 0.94921875\n",
            "Epoch: 330 Cost: 0.231509 Dev acc: 0.76953125 Train acc: 0.93359375\n",
            "Epoch: 335 Cost: 0.228290692 Dev acc: 0.77734375 Train acc: 0.953125\n",
            "Epoch: 340 Cost: 0.24484022 Dev acc: 0.70703125 Train acc: 0.8359375\n",
            "Epoch: 345 Cost: 0.244086564 Dev acc: 0.7578125 Train acc: 0.92578125\n",
            "Epoch: 350 Cost: 0.21609728 Dev acc: 0.7734375 Train acc: 0.953125\n",
            "Epoch: 355 Cost: 0.286482513 Dev acc: 0.765625 Train acc: 0.9609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6JEOOTmNnrI"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline \n",
        "\n",
        "def plot_learning_curve(par_values, train_scores, dev_scores, title=\"Learning Curve\", xlab=\"\", ylab=\"Accuracy\", ylim=None):\n",
        "    \"\"\"\n",
        "    Generate a simple plot of the test and training learning curve.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    par_values : list of checked values of the current parameter.\n",
        "    \n",
        "    train_scores : list of scores obtained in training set (same length as par_values).\n",
        "    \n",
        "    test_scores : list of scores obtained in dev set (same length as par_values)\n",
        "\n",
        "    title : string\n",
        "        Title for the chart.\n",
        "\n",
        "    ylim : tuple, shape (ymin, ymax), optional\n",
        "        Defines minimum and maximum yvalues plotted.\n",
        "    \"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(xlab)\n",
        "    plt.ylabel(ylab)\n",
        "    \n",
        "    plt.grid()\n",
        "    plt.plot(par_values, train_scores, color=\"r\",label=\"Training score\")\n",
        "    plt.plot(par_values, dev_scores, color=\"g\", label=\"Dev score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaPOrYk02uMb"
      },
      "source": [
        "plt = plot_learning_curve(epochs, train_acc, dev_acc, xlab=\"Epoch\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuyZiJ_YSpxG"
      },
      "source": [
        "#  Atribution:\n",
        "Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"
      ]
    }
  ]
}