{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. RNN based classifier.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxcogNx1spwp"
      },
      "source": [
        "# Lab3: Sentiment, but slower!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7huCqsRspww"
      },
      "source": [
        "In this assignment, you'll implement an **RNN-based sentence classifier**. Plain ol' RNNs aren't very good at sentiment classification, and they're very picky about things like learning rates. However, they're the foundation for things like LSTMs, which we'll learn about next week, and which *are* quite useful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcEY8ElAspw4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2LXIRspspw_"
      },
      "source": [
        "First, let's load the data as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvjTs7K1t17g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f9d3cac-470c-43b9-b367-918c32f4459a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWRmPqvTspxB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "481fc85b-8434-4e70-f89c-8816c4dcdb1b"
      },
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "# Let's do 2-way positive/negative classification instead of 5-way\n",
        "def load_sst_data(path,\n",
        "                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        for i, line in enumerate(f): \n",
        "            example = {}\n",
        "            example['label'] = easy_label_map[int(line[1])]\n",
        "            if example['label'] is None:\n",
        "                continue\n",
        "            \n",
        "            # Strip out the parse information and the phrase labels--\n",
        "            # ---we don't need those here\n",
        "            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n",
        "            example['text'] = text[1:]\n",
        "            data.append(example)\n",
        "    \n",
        "    random.seed(1)\n",
        "    random.shuffle(data)\n",
        "    return data\n",
        "   \n",
        "sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n",
        "training_set = load_sst_data(sst_home + '/train.txt')\n",
        "dev_set = load_sst_data(sst_home + '/dev.txt')\n",
        "test_set = load_sst_data(sst_home + '/test.txt')\n",
        "\n",
        "print('Training size: {}'.format(len(training_set)))\n",
        "print('Dev size: {}'.format(len(dev_set)))\n",
        "print('Test size: {}'.format(len(test_set)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size: 6920\n",
            "Dev size: 872\n",
            "Test size: 1821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaoqE0D7spxL"
      },
      "source": [
        "Next, we'll convert the data to __index vectors__.\n",
        "\n",
        "To simplify your implementation, we'll use a __fixed unrolling length of 20__. In the conversion process, we'll cut off excess words (towards the left/start end of the sentence), pad short sentences (to the left) with a special word symbol `<PAD>`, and mark out-of-vocabulary words with `<UNK>`, for unknown. As in the previous assignment, we'll use a very small vocabulary for this assignment, so you'll see `<UNK>` often."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7d0AMI6spxQ"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "def sentence_to_padded_index_sequence(datasets):\n",
        "    '''Annotates datasets with feature vectors.'''\n",
        "    \n",
        "    PADDING = \"<PAD>\"\n",
        "    UNKNOWN = \"<UNK>\"\n",
        "    SEQ_LEN = 20\n",
        "    \n",
        "    # Extract vocabulary\n",
        "    def tokenize(string):\n",
        "        return string.lower().split()\n",
        "    \n",
        "    word_counter = collections.Counter()\n",
        "    for example in datasets[0]:\n",
        "        word_counter.update(tokenize(example['text']))\n",
        "    \n",
        "    vocabulary = set([word for word in word_counter if word_counter[word] > 10])\n",
        "    vocabulary = list(vocabulary)\n",
        "    vocabulary = [PADDING, UNKNOWN] + vocabulary\n",
        "        \n",
        "    word_indices = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "    indices_to_words = {v: k for k, v in word_indices.items()}\n",
        "        \n",
        "    for i, dataset in enumerate(datasets):\n",
        "        for example in dataset:\n",
        "            example['index_sequence'] = np.zeros((SEQ_LEN), dtype=np.int32)\n",
        "            \n",
        "            token_sequence = tokenize(example['text'])\n",
        "            padding = SEQ_LEN - len(token_sequence)\n",
        "            \n",
        "            for i in range(SEQ_LEN):\n",
        "                if i >= padding:\n",
        "                    if token_sequence[i - padding] in word_indices:\n",
        "                        index = word_indices[token_sequence[i - padding]]\n",
        "                    else:\n",
        "                        index = word_indices[UNKNOWN]\n",
        "                else:\n",
        "                    index = word_indices[PADDING]\n",
        "                example['index_sequence'][i] = index\n",
        "    return indices_to_words, word_indices\n",
        "    \n",
        "indices_to_words, word_indices = sentence_to_padded_index_sequence([training_set, dev_set, test_set])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Apdx7iJospxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42964c5b-44b4-4602-fe82-caf0179aff6a"
      },
      "source": [
        "print(training_set[18])\n",
        "print(len(word_indices))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'label': 1, 'text': 'As the dominant Christine , Sylvie Testud is icily brilliant .', 'index_sequence': array([   0,    0,    0,    0,    0,    0,    0,    0,    0,  615,  634,\n",
            "          1,    1,  669,    1,    1,  348,    1, 1172,  806], dtype=int32)}\n",
            "1250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_qTijrGspxf"
      },
      "source": [
        "def evaluate_classifier(classifier, eval_set):\n",
        "    correct = 0\n",
        "    hypotheses = classifier(eval_set)\n",
        "    for i, example in enumerate(eval_set):\n",
        "        hypothesis = hypotheses[i]\n",
        "        if hypothesis == example['label']:\n",
        "            correct += 1        \n",
        "    return correct / float(len(eval_set))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLcqDcjvspxm"
      },
      "source": [
        "## Assignments: Building the RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uC9N01Mspxq"
      },
      "source": [
        "Replace the TODOs in the code below to make RNN work. If it's set up properly, it should reach dev set accuracy of about 0.70 within 500 epochs with the given hyperparameters.\n",
        "\n",
        "You will find 3 TODOs in the code.\n",
        "\n",
        "### TODO 1:\n",
        "\n",
        "- You have to define the RNN parameters (attribute *self.dim* sets dimmension of hidden state). \n",
        "\n",
        "- (Hint) The paremters take input's embedding (*self.embedding_dim*) and the previous hidden state (*self.dim*) and provides the current hidden state (*self.dim*).\n",
        "\n",
        "### TODO 2:\n",
        "\n",
        "- Write a (very short) Python function that defines one step of an RNN. (Hint) In each step current input and previous hidden states are involved. \n",
        "\n",
        "- Recall from slides: $f(h_{t-1}, p_t) = tanh(W[h_{t-1};p_t])$. Note that input $x$ at time step $t$ is *translated* to its embedding representation. \n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?id=1VNI--El3renuefGD0R7AOlcxI4ycLj4V)\n",
        "\n",
        "\n",
        "### TODO 3:\n",
        "\n",
        "- Unroll the RNN using a *for* loop, and obtain the sentence representation with the final hidden state.\n",
        "\n",
        "- (Hint) Note that we are vectorizing the whole minibatch. That is, in each step we are processing all the examples in the batch together in one go. Try to understand the following two code lines:\n",
        "\n",
        "   $\\rightarrow$ ``self.x_slices = tf.split(self.x, self.sequence_length, 1)``\n",
        "   \n",
        "   $\\rightarrow$ ``self.h_zero = tf.zeros([self.batch_size, self.dim])``\n",
        "   \n",
        "- (Hint) It might be a good idea to reshape (tf.reshape) the tensor at step t in a single tensor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tG8H7bYgspxw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "bd636aed-f7d9-4c23-bc3c-ba4d8b2fca5a"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.7.0'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piE_Cr6zspx6"
      },
      "source": [
        "class RNNSentimentClassifier:\n",
        "    def __init__(self, vocab_size, sequence_length):\n",
        "        # Define the hyperparameters\n",
        "        self.learning_rate = 0.2  # Should be about right\n",
        "        self.training_epochs = 500  # How long to train for - chosen to fit within class time\n",
        "        self.display_epoch_freq = 5  # How often to test and print out statistics\n",
        "        self.dim = 24  # The dimension of the hidden state of the RNN\n",
        "        self.embedding_dim = 8  # The dimension of the learned word embeddings\n",
        "        self.batch_size = 256  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n",
        "        self.vocab_size = vocab_size  # Defined by the file reader above\n",
        "        self.sequence_length = sequence_length  # Defined by the file reader above\n",
        "        self.l2_lambda = 0.001\n",
        "        \n",
        "        self.trainable_variables = []\n",
        "\n",
        "        # Define the parameters\n",
        "        self.E = tf.Variable(tf.random.normal([self.vocab_size, self.embedding_dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.E)\n",
        "        \n",
        "        self.W_cl = tf.Variable(tf.random.normal([self.dim, 2], stddev=0.1))\n",
        "        self.b_cl = tf.Variable(tf.random.normal([2], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_cl)\n",
        "        self.trainable_variables.append(self.b_cl)\n",
        "        \n",
        "        # TODO 1: Define the RNN parameters\n",
        "        \n",
        "        self.W_recurrent = tf.Variable(tf.random.normal([self.embedding_dim+self.dim, self.dim], stddev=0.1))\n",
        "        # self.b_recurrent = tf.Variable(tf.random.normal([self.dim], stddev=0.1))\n",
        "        self.trainable_variables.append(self.W_recurrent)\n",
        "        # self.trainable_variables.append(self.b_cl)\n",
        "        \n",
        "    def model(self,x):\n",
        "        # Split up the inputs into individual tensors\n",
        "        self.x_slices = tf.split(x, self.sequence_length, 1)\n",
        "    \n",
        "        # Define the start state of the RNN\n",
        "        self.h_zero = tf.zeros([self.batch_size, self.dim])  \n",
        "        \n",
        "        # TODO 2: Write a (very short) Python function that defines one step of an RNN\n",
        "        def step(x, h_prev):\n",
        "          h_minus_t = tf.concat([h_prev, x], axis=1) # concatenate the previous_h (some embedding) with the current word in the axis 1, so (256, 24) * (256, 8) where 24 and 8 are axis 1. \n",
        "\n",
        "\n",
        "          h = tf.nn.tanh(tf.matmul(h_minus_t, self.W_recurrent))\n",
        "\n",
        "          return h\n",
        "        \n",
        "\n",
        "\n",
        "        # TODO 3: Unroll the RNN using a for loop, and obtain the sentence representation with the final hidden state\n",
        "\n",
        "        h_step = self.h_zero\n",
        "        for current_word in self.x_slices:\n",
        "          current_word = tf.reshape(current_word, self.batch_size)\n",
        "          x = tf.nn.embedding_lookup(self.E, current_word)\n",
        "          h_step = step(x,h_step)\n",
        "\n",
        "        sentence_representation = h_step\n",
        "\n",
        "\n",
        "\n",
        "        # Compute the logits using one last linear layer\n",
        "        logits = tf.matmul(sentence_representation, self.W_cl) + self.b_cl\n",
        "        return logits\n",
        "\n",
        "    def train(self, training_data, dev_set):\n",
        "        def get_minibatch(dataset, start_index, end_index):\n",
        "            indices = range(start_index, end_index)\n",
        "            vectors = np.vstack([dataset[i]['index_sequence'] for i in indices])\n",
        "            labels = [dataset[i]['label'] for i in indices]\n",
        "            return vectors, labels\n",
        "      \n",
        "        print('Training.')\n",
        "\n",
        "        # Training cycle\n",
        "        for epoch in range(self.training_epochs):\n",
        "            random.shuffle(training_set)\n",
        "            avg_cost = 0.\n",
        "            total_batch = int(len(training_set) / self.batch_size)\n",
        "            \n",
        "            # Loop over all batches in epoch\n",
        "            for i in range(total_batch):\n",
        "                # Assemble a minibatch of the next B examples\n",
        "                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n",
        "                                                                    self.batch_size * i, \n",
        "                                                                    self.batch_size * (i + 1))\n",
        "\n",
        "                # Run the optimizer to take a gradient step, and also fetch the value of the \n",
        "                # cost function for logging\n",
        "                with tf.GradientTape() as tape:\n",
        "                  logits = self.model(minibatch_vectors)\n",
        "                \n",
        "                  # Define the L2 cost\n",
        "                  self.l2_cost = self.l2_lambda * (tf.reduce_sum(tf.square(self.W_recurrent)) +\n",
        "                                                  tf.reduce_sum(tf.square(self.W_cl)))\n",
        "\n",
        "                  # Define the cost function (here, the softmax exp and sum are built in)\n",
        "                  total_cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=minibatch_labels, logits=logits) + self.l2_cost)\n",
        "        \n",
        "                # This  performs the main SGD update equation with gradient clipping\n",
        "                optimizer = tf.optimizers.SGD(self.learning_rate)\n",
        "                gradients = tape.gradient(total_cost, self.trainable_variables)\n",
        "                gvs = zip(gradients, self.trainable_variables)\n",
        "                capped_gvs = [(tf.clip_by_norm(grad, 5.0), var) for grad, var in gvs if grad is not None]\n",
        "                optimizer.apply_gradients(capped_gvs)\n",
        "                                                                            \n",
        "                # Compute average loss\n",
        "                avg_cost += total_cost / total_batch\n",
        "                \n",
        "            # Display some statistics about the step\n",
        "            # Evaluating only one batch worth of data -- simplifies implementation slightly\n",
        "            if (epoch+1) % self.display_epoch_freq == 0:\n",
        "                tf.print(\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n",
        "                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:256]), \\\n",
        "                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:256]))  \n",
        "    \n",
        "    def classify(self, examples):\n",
        "        # This classifies a list of examples\n",
        "        vectors = np.vstack([example['index_sequence'] for example in examples])\n",
        "        logits = self.model(vectors)\n",
        "        return np.argmax(logits, axis=1)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26SBs4iespyE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0af854-9af5-4428-f588-0b3a17fea158"
      },
      "source": [
        "classifier = RNNSentimentClassifier(len(word_indices), 20)\n",
        "classifier.train(training_set, dev_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training.\n",
            "Epoch: 5 Cost: 0.699703515 Dev acc: 0.5546875 Train acc: 0.50390625\n",
            "Epoch: 10 Cost: 0.698780239 Dev acc: 0.5546875 Train acc: 0.5703125\n",
            "Epoch: 15 Cost: 0.698094904 Dev acc: 0.5546875 Train acc: 0.515625\n",
            "Epoch: 20 Cost: 0.697534084 Dev acc: 0.5546875 Train acc: 0.55078125\n",
            "Epoch: 25 Cost: 0.696857154 Dev acc: 0.5625 Train acc: 0.53515625\n",
            "Epoch: 30 Cost: 0.696355462 Dev acc: 0.5625 Train acc: 0.53125\n",
            "Epoch: 35 Cost: 0.69575578 Dev acc: 0.5546875 Train acc: 0.49609375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# evaluate_classifier(classifier.classify, test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "VcK3NCr2Xp1G",
        "outputId": "1268a5e9-5699-412a-e487-b67380382dac"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8677a9ba6459>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-2bab061d5d9b>\u001b[0m in \u001b[0;36mevaluate_classifier\u001b[0;34m(classifier, eval_set)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mhypotheses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-a7481e117819>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m# This classifies a list of examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index_sequence'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mexample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-a7481e117819>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mh_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_zero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcurrent_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_slices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m           \u001b[0mcurrent_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m           \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_word\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[0mh_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7106\u001b[0m   \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7107\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Input to reshape is a tensor with 1821 values, but the requested shape has 256 [Op:Reshape]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjBUT-D2spyT"
      },
      "source": [
        "# Atribution:\n",
        "Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"
      ]
    }
  ]
}