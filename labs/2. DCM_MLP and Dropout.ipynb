{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. MLP and Dropout.ipynb","provenance":[{"file_id":"1mkhICIQba8Oz0Iu3FKw1EGZTIzdFHsXv","timestamp":1541614347511}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"HzzMSlqjSGBg"},"source":["# Lab2: MLPs and Dropout"]},{"cell_type":"markdown","metadata":{"id":"oJof5oljSGBk"},"source":["First, let's load the data as before."]},{"cell_type":"code","metadata":{"id":"e8fdLvE4Ygyq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642100457165,"user_tz":-60,"elapsed":20917,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}},"outputId":"924cb28a-aca5-4799-a60f-0e25c02aaf67"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"OoRAgDVzSGBn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642100807981,"user_tz":-60,"elapsed":391,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}},"outputId":"73ee7c66-8164-46c7-efb5-43ccff9775b6"},"source":["# Load the data\n","import re\n","import random\n","\n","# Let's do 2-way positive/negative classification instead of 5-way    \n","def load_sst_data(path,\n","                  easy_label_map={0:0, 1:0, 2:None, 3:1, 4:1}):\n","    data = []\n","    with open(path) as f:\n","        for i, line in enumerate(f): \n","            example = {}\n","            example['label'] = easy_label_map[int(line[1])]\n","            if example['label'] is None:\n","                continue\n","            \n","            # Strip out the parse information and the phrase labels--\n","            # ---we don't need those here\n","            text = re.sub(r'\\s*(\\(\\d)|(\\))\\s*', '', line)\n","            example['text'] = text[1:]\n","            data.append(example)\n","    random.seed(1)\n","    random.shuffle(data)\n","    return data\n","\n","sst_home = 'drive/My Drive/Colab Notebooks/dl4nlp_labs/data/trees/'\n","training_set = load_sst_data(sst_home + 'train.txt')\n","dev_set = load_sst_data(sst_home + 'dev.txt')\n","test_set = load_sst_data(sst_home + 'test.txt')\n","\n","print('Training size: {}'.format(len(training_set)))\n","print('Dev size: {}'.format(len(dev_set)))\n","print('Test size: {}'.format(len(test_set)))\n","\n"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Training size: 6920\n","Dev size: 872\n","Test size: 1821\n"]}]},{"cell_type":"markdown","metadata":{"id":"RvlDOvE9SGBy"},"source":["And extract bag-of-words feature vectors. For speed, we'll only use words that appear at least 10 times in the training set, leaving us with $|V|=1254$."]},{"cell_type":"code","metadata":{"id":"ZzJYCAobSGB0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642100808897,"user_tz":-60,"elapsed":485,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}},"outputId":"7d4d0fb8-34b4-49c1-86d1-5e57a8434de5"},"source":["import collections\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","def feature_function(datasets):\n","    '''Annotates datasets with feature vectors.'''\n","    \n","    # Extract vocabulary\n","    def tokenize(string):\n","        return string.split()\n","    \n","    word_counter = collections.Counter()\n","    for example in datasets[0]:\n","        word_counter.update(tokenize(example['text']))\n","    \n","    vocabulary = set([word for word in word_counter if word_counter[word] > 3])\n","                                \n","    feature_names = set()\n","    for i, dataset in enumerate(datasets):\n","        for example in dataset:\n","            example['features'] = collections.defaultdict(float)\n","            \n","            # Extract features (by name) for one example\n","            word_counter = collections.Counter(tokenize(example['text']))\n","            for x in word_counter.items():\n","                if x[0] in vocabulary:\n","                    example[\"features\"][\"word_count_for_\" + x[0]] = x[1]\n","            \n","            feature_names.update(example['features'].keys())\n","                            \n","    # By now, we know what all the features will be, so we can\n","    # assign indices to them.\n","    feature_indices = dict(zip(feature_names, range(len(feature_names))))\n","    indices_to_features = {v: k for k, v in feature_indices.items()}\n","    dim = len(feature_indices)\n","                \n","    # Now we create actual vectors from those indices.\n","    for dataset in datasets:\n","        for example in dataset:\n","            example['vector'] = np.zeros((dim))\n","            for feature in example['features']:\n","                example['vector'][feature_indices[feature]] = example['features'][feature]\n","    return indices_to_features, dim\n","    \n","indices_to_features, dim = feature_function([training_set, dev_set, test_set])\n","\n","print('Vocabulary size: {}'.format(dim))\n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 3750\n"]}]},{"cell_type":"markdown","metadata":{"id":"gUlD7zgkSGB7"},"source":["And define a batch evalution function."]},{"cell_type":"code","metadata":{"id":"A533GAkvSGB9","executionInfo":{"status":"ok","timestamp":1642100825528,"user_tz":-60,"elapsed":300,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}}},"source":["def evaluate_classifier(classifier, eval_set):\n","    correct = 0\n","    hypotheses = classifier(eval_set)\n","    for i, example in enumerate(eval_set):\n","        hypothesis = hypotheses[i]\n","        if hypothesis == example['label']:\n","            correct += 1        \n","    return correct / float(len(eval_set))"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H0Qn58-YSGCD"},"source":["## Assignments\n","\n","Now for the fun part! The below should be a working implementation of logistic regression in TensorFlow.\n","\n","### Part One:\n","\n","Modify it to turn it into an MLP with two ReLU hidden layers of 50 dimensions.\n","\n","Keep in mind that initializing weight matrices with zeros causes problems in deep neural networks trained by SGD. (Why?) You should use tf.random.normal instead, with stddev=0.1.\n","\n","If your model works, it should be able to overfit, reaching about 90% accuracy *on the training set* in the first 100 epochs.\n","\n","### Part Two:\n","\n","After each hidden layer, add dropout with a 80% keep rate (20% of drop rate). You're welcome to use `tf.nn.dropout`.\n","\n","Remember that dropout behaves differently at training time and at test time. This is not automatic. You can implement in various ways, but an easy way can be this:\n","\n","- Hint: Treat the dropout rate as an input to the model, just like `x`. At training time, feed it a value of `0.2`, at test time, feed it a value of `0.0`. You can explore different dropout values.\n","\n","If dropout works, your model should overfit less, but should still perform about as well (or, hopefully, better) on the dev set."]},{"cell_type":"code","metadata":{"id":"CMJeulPiSGCF","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1642100825875,"user_tz":-60,"elapsed":6,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}},"outputId":"e70646d9-59fa-4e1a-dc39-e9e2e5bac2d6"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","tf.__version__"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.7.0'"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"EyS-IkAYSGCL","executionInfo":{"status":"ok","timestamp":1642100826734,"user_tz":-60,"elapsed":496,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}}},"source":["class logistic_regression_classifier:\n","    def __init__(self, dim):\n","        # Define the hyperparameters\n","        self.learning_rate = 0.3  # Should be about right\n","        self.training_epochs = 100  # How long to train for - chosen to fit within class time\n","        self.display_epoch_freq = 1  # How often to test and print out statistics\n","        self.dim = dim  # The number of features\n","        self.batch_size = 150  # Somewhat arbitrary - can be tuned, but often tune for speed, not accuracy\n","        \n","        # TODO: Use these.\n","        self.hidden_layer_sizes = [90, 90]\n","        self.rate = 0.2\n","\n","        # TODO: Overwrite this section\n","        ### Start of model definition ###\n","        self.trainable_variables = []\n","         # Define (most of) the model\n","        '''Variables'''\n","        #Output layer\n","        self.W0 = tf.Variable(tf.random.normal([self.dim, 50], stddev=0.02), dtype='float32')\n","        self.b0 = tf.Variable(tf.random.normal([50], stddev=0.002), dtype='float32')\n","        self.trainable_variables.append(self.W0)\n","        self.trainable_variables.append(self.b0)\n","        \n","        self.W1 = tf.Variable(tf.random.normal([50, 100], stddev=0.02), dtype='float32')\n","        self.b1 = tf.Variable(tf.random.normal([100], stddev=0.02), dtype='float32')\n","        self.trainable_variables.append(self.W1)\n","        self.trainable_variables.append(self.b1)\n","        \n","        self.W2 = tf.Variable(tf.random.normal([100, 50], stddev=0.02), dtype='float32')\n","        self.b2 = tf.Variable(tf.random.normal([50], stddev=0.02), dtype='float32')\n","        self.trainable_variables.append(self.W2)\n","        self.trainable_variables.append(self.b2)\n","\n","        self.W3 = tf.Variable(tf.random.normal([50, 2], stddev=0.02), dtype='float32')\n","        self.b3 = tf.Variable(tf.random.normal([2], stddev=0.02), dtype='float32')\n","        self.trainable_variables.append(self.W2)\n","        self.trainable_variables.append(self.b2)\n","\n","\n","        # TODO: Overwrite this section\n","\n","    def model(self,x, dropout=0):\n","        '''Training Computation'''\n","        # TODO: Overwrite this section\n","        #Output layer activation\n","        h0 = tf.matmul(x, self.W0) + self.b0\n","        h0 = tf.nn.relu(h0)\n","        h0 = tf.nn.dropout(h0, rate=self.rate, seed=1)\n","\n","        h1 = tf.matmul(h0, self.W1) + self.b1\n","        h1 = tf.nn.relu(h1)\n","        h1 = tf.nn.dropout(h1, rate=self.rate, seed=1)\n","\n","        h2 = tf.matmul(h1, self.W2) + self.b2\n","        h2 = tf.nn.relu(h2)\n","        h2 = tf.nn.dropout(h2, rate=self.rate, seed=1)\n","        \n","        logits = tf.matmul(h2, self.W3) + self.b3\n","\n","        \n","        # TODO: Overwrite this section\n","        ### End of model definition ###\n","        return logits\n","     \n","\n","    def train(self, training_data, dev_set):\n","        def get_minibatch(dataset, start_index, end_index):\n","            indices = range(start_index, end_index)\n","            vectors = np.float32(np.vstack([dataset[i]['vector'] for i in indices]))\n","            labels = [dataset[i]['label'] for i in indices]\n","            return vectors, labels\n","      \n","        print ('Training.')\n","\n","        # Training cycle\n","        for epoch in range(self.training_epochs):\n","            random.shuffle(training_set)\n","            avg_cost = 0.\n","            total_batch = int(len(training_set) / self.batch_size)\n","            # Loop over all batches in epoch\n","            for i in range(total_batch):\n","                # Assemble a minibatch of the next B examples\n","                minibatch_vectors, minibatch_labels = get_minibatch(training_set, \n","                                                                    self.batch_size * i, \n","                                                                    self.batch_size * (i + 1))\n","                # Run the optimizer to take a gradient step, and also fetch the value of the \n","                # cost function for logging\n","                with tf.GradientTape() as tape:\n","                  logits = self.model(minibatch_vectors)\n","                  # Define the cost function (here, the exp and sum are built in)\n","                  cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=minibatch_labels))\n","                gradients = tape.gradient(cost, self.trainable_variables)\n","                optimizer = tf.optimizers.SGD(self.learning_rate)\n","                optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n","                # Compute average loss\n","                avg_cost += cost / total_batch\n","                \n","                # Display some statistics about the step\n","            if (epoch+1) % self.display_epoch_freq == 0:\n","                tf.print (\"Epoch:\", (epoch+1), \"Cost:\", avg_cost, \\\n","                    \"Dev acc:\", evaluate_classifier(self.classify, dev_set[0:500]), \\\n","                    \"Train acc:\", evaluate_classifier(self.classify, training_set[0:500]))\n","    \n","    def classify(self, examples):\n","        # This classifies a list of examples\n","        vectors = np.float32(np.vstack([example['vector'] for example in examples]))\n","        logits = self.model(vectors, dropout=0.0)\n","        return np.argmax(logits, axis=1)"],"execution_count":33,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sU7c7S6rSGCT"},"source":["Now let's train it."]},{"cell_type":"code","metadata":{"id":"kymCD3LkSGCW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642100902197,"user_tz":-60,"elapsed":74502,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}},"outputId":"1bb7143d-eac8-4462-b232-400a70befa99"},"source":["classifier = logistic_regression_classifier(dim)\n","classifier.train(training_set, dev_set)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Training.\n","Epoch: 1 Cost: 0.694617271 Dev acc: 0.472 Train acc: 0.506\n","Epoch: 2 Cost: 0.694414198 Dev acc: 0.472 Train acc: 0.478\n","Epoch: 3 Cost: 0.69418329 Dev acc: 0.472 Train acc: 0.488\n","Epoch: 4 Cost: 0.694010258 Dev acc: 0.472 Train acc: 0.45\n","Epoch: 5 Cost: 0.693822384 Dev acc: 0.472 Train acc: 0.456\n","Epoch: 6 Cost: 0.693665385 Dev acc: 0.472 Train acc: 0.474\n","Epoch: 7 Cost: 0.693590462 Dev acc: 0.472 Train acc: 0.434\n","Epoch: 8 Cost: 0.693367839 Dev acc: 0.476 Train acc: 0.476\n","Epoch: 9 Cost: 0.693350673 Dev acc: 0.488 Train acc: 0.502\n","Epoch: 10 Cost: 0.693215787 Dev acc: 0.576 Train acc: 0.522\n","Epoch: 11 Cost: 0.693320096 Dev acc: 0.478 Train acc: 0.496\n","Epoch: 12 Cost: 0.693046808 Dev acc: 0.502 Train acc: 0.548\n","Epoch: 13 Cost: 0.692935944 Dev acc: 0.466 Train acc: 0.522\n","Epoch: 14 Cost: 0.692780435 Dev acc: 0.55 Train acc: 0.478\n","Epoch: 15 Cost: 0.692886651 Dev acc: 0.508 Train acc: 0.482\n","Epoch: 16 Cost: 0.692639589 Dev acc: 0.512 Train acc: 0.518\n","Epoch: 17 Cost: 0.692610562 Dev acc: 0.54 Train acc: 0.55\n","Epoch: 18 Cost: 0.692525148 Dev acc: 0.528 Train acc: 0.522\n","Epoch: 19 Cost: 0.69256115 Dev acc: 0.53 Train acc: 0.498\n","Epoch: 20 Cost: 0.692577183 Dev acc: 0.524 Train acc: 0.566\n","Epoch: 21 Cost: 0.692504942 Dev acc: 0.532 Train acc: 0.514\n","Epoch: 22 Cost: 0.69238168 Dev acc: 0.532 Train acc: 0.542\n","Epoch: 23 Cost: 0.692647099 Dev acc: 0.52 Train acc: 0.53\n","Epoch: 24 Cost: 0.692038953 Dev acc: 0.538 Train acc: 0.546\n","Epoch: 25 Cost: 0.692129 Dev acc: 0.546 Train acc: 0.51\n","Epoch: 26 Cost: 0.692523956 Dev acc: 0.526 Train acc: 0.526\n","Epoch: 27 Cost: 0.692109525 Dev acc: 0.52 Train acc: 0.514\n","Epoch: 28 Cost: 0.692212582 Dev acc: 0.522 Train acc: 0.544\n","Epoch: 29 Cost: 0.692255676 Dev acc: 0.53 Train acc: 0.512\n","Epoch: 30 Cost: 0.692030549 Dev acc: 0.528 Train acc: 0.57\n","Epoch: 31 Cost: 0.692338228 Dev acc: 0.536 Train acc: 0.49\n","Epoch: 32 Cost: 0.691982269 Dev acc: 0.524 Train acc: 0.55\n","Epoch: 33 Cost: 0.691954136 Dev acc: 0.526 Train acc: 0.476\n","Epoch: 34 Cost: 0.692329407 Dev acc: 0.528 Train acc: 0.518\n","Epoch: 35 Cost: 0.692180336 Dev acc: 0.528 Train acc: 0.54\n","Epoch: 36 Cost: 0.691789389 Dev acc: 0.516 Train acc: 0.532\n","Epoch: 37 Cost: 0.692044675 Dev acc: 0.534 Train acc: 0.54\n","Epoch: 38 Cost: 0.691614032 Dev acc: 0.53 Train acc: 0.496\n","Epoch: 39 Cost: 0.691234767 Dev acc: 0.53 Train acc: 0.538\n","Epoch: 40 Cost: 0.691000223 Dev acc: 0.528 Train acc: 0.552\n","Epoch: 41 Cost: 0.691201925 Dev acc: 0.538 Train acc: 0.528\n","Epoch: 42 Cost: 0.690457582 Dev acc: 0.53 Train acc: 0.55\n","Epoch: 43 Cost: 0.689742506 Dev acc: 0.528 Train acc: 0.494\n","Epoch: 44 Cost: 0.688515723 Dev acc: 0.546 Train acc: 0.502\n","Epoch: 45 Cost: 0.68744874 Dev acc: 0.536 Train acc: 0.532\n","Epoch: 46 Cost: 0.685912311 Dev acc: 0.536 Train acc: 0.52\n","Epoch: 47 Cost: 0.683340549 Dev acc: 0.564 Train acc: 0.56\n","Epoch: 48 Cost: 0.680497348 Dev acc: 0.588 Train acc: 0.534\n","Epoch: 49 Cost: 0.67769444 Dev acc: 0.634 Train acc: 0.568\n","Epoch: 50 Cost: 0.673616707 Dev acc: 0.658 Train acc: 0.594\n","Epoch: 51 Cost: 0.668507755 Dev acc: 0.64 Train acc: 0.632\n","Epoch: 52 Cost: 0.662385106 Dev acc: 0.666 Train acc: 0.612\n","Epoch: 53 Cost: 0.654905856 Dev acc: 0.666 Train acc: 0.654\n","Epoch: 54 Cost: 0.645679653 Dev acc: 0.67 Train acc: 0.666\n","Epoch: 55 Cost: 0.632558823 Dev acc: 0.658 Train acc: 0.64\n","Epoch: 56 Cost: 0.61775136 Dev acc: 0.7 Train acc: 0.714\n","Epoch: 57 Cost: 0.602825 Dev acc: 0.722 Train acc: 0.742\n","Epoch: 58 Cost: 0.583326221 Dev acc: 0.714 Train acc: 0.78\n","Epoch: 59 Cost: 0.557029307 Dev acc: 0.704 Train acc: 0.732\n","Epoch: 60 Cost: 0.547011077 Dev acc: 0.702 Train acc: 0.73\n","Epoch: 61 Cost: 0.517449439 Dev acc: 0.78 Train acc: 0.78\n","Epoch: 62 Cost: 0.508103549 Dev acc: 0.724 Train acc: 0.742\n","Epoch: 63 Cost: 0.508124232 Dev acc: 0.742 Train acc: 0.79\n","Epoch: 64 Cost: 0.501369 Dev acc: 0.758 Train acc: 0.81\n","Epoch: 65 Cost: 0.476778597 Dev acc: 0.768 Train acc: 0.836\n","Epoch: 66 Cost: 0.479394823 Dev acc: 0.756 Train acc: 0.822\n","Epoch: 67 Cost: 0.458602846 Dev acc: 0.758 Train acc: 0.836\n","Epoch: 68 Cost: 0.446162939 Dev acc: 0.69 Train acc: 0.714\n","Epoch: 69 Cost: 0.44281432 Dev acc: 0.726 Train acc: 0.812\n","Epoch: 70 Cost: 0.43357873 Dev acc: 0.75 Train acc: 0.832\n","Epoch: 71 Cost: 0.406335562 Dev acc: 0.774 Train acc: 0.84\n","Epoch: 72 Cost: 0.392851204 Dev acc: 0.636 Train acc: 0.628\n","Epoch: 73 Cost: 0.399767935 Dev acc: 0.786 Train acc: 0.894\n","Epoch: 74 Cost: 0.372389972 Dev acc: 0.774 Train acc: 0.91\n","Epoch: 75 Cost: 0.358528644 Dev acc: 0.77 Train acc: 0.83\n","Epoch: 76 Cost: 0.37340939 Dev acc: 0.716 Train acc: 0.798\n","Epoch: 77 Cost: 0.347119451 Dev acc: 0.756 Train acc: 0.912\n","Epoch: 78 Cost: 0.332780898 Dev acc: 0.714 Train acc: 0.816\n","Epoch: 79 Cost: 0.330882072 Dev acc: 0.692 Train acc: 0.78\n","Epoch: 80 Cost: 0.316643804 Dev acc: 0.738 Train acc: 0.862\n","Epoch: 81 Cost: 0.318852276 Dev acc: 0.782 Train acc: 0.922\n","Epoch: 82 Cost: 0.290967852 Dev acc: 0.768 Train acc: 0.932\n","Epoch: 83 Cost: 0.274855137 Dev acc: 0.784 Train acc: 0.93\n","Epoch: 84 Cost: 0.287879378 Dev acc: 0.772 Train acc: 0.928\n","Epoch: 85 Cost: 0.284658611 Dev acc: 0.76 Train acc: 0.914\n","Epoch: 86 Cost: 0.238770634 Dev acc: 0.658 Train acc: 0.688\n","Epoch: 87 Cost: 0.252568841 Dev acc: 0.744 Train acc: 0.898\n","Epoch: 88 Cost: 0.255480021 Dev acc: 0.77 Train acc: 0.936\n","Epoch: 89 Cost: 0.237094641 Dev acc: 0.772 Train acc: 0.916\n","Epoch: 90 Cost: 0.252580673 Dev acc: 0.758 Train acc: 0.944\n","Epoch: 91 Cost: 0.221945509 Dev acc: 0.782 Train acc: 0.966\n","Epoch: 92 Cost: 0.240193874 Dev acc: 0.75 Train acc: 0.902\n","Epoch: 93 Cost: 0.167883143 Dev acc: 0.766 Train acc: 0.966\n","Epoch: 94 Cost: 0.19197458 Dev acc: 0.734 Train acc: 0.906\n","Epoch: 95 Cost: 0.143089533 Dev acc: 0.768 Train acc: 0.972\n","Epoch: 96 Cost: 0.211159304 Dev acc: 0.776 Train acc: 0.952\n","Epoch: 97 Cost: 0.188382074 Dev acc: 0.77 Train acc: 0.962\n","Epoch: 98 Cost: 0.249941945 Dev acc: 0.73 Train acc: 0.826\n","Epoch: 99 Cost: 0.146947071 Dev acc: 0.732 Train acc: 0.93\n","Epoch: 100 Cost: 0.189724073 Dev acc: 0.784 Train acc: 0.966\n"]}]},{"cell_type":"markdown","metadata":{"id":"F_6kMcOhSGCe"},"source":["And evaluate it."]},{"cell_type":"code","metadata":{"id":"ZkTmNJpCSGCf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642100910653,"user_tz":-60,"elapsed":341,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}},"outputId":"58422ac8-1dcc-498e-8fab-b71f20d70e57"},"source":["evaluate_classifier(classifier.classify, test_set)\n"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7808896210873146"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"kjBUT-D2spyT"},"source":["# Atribution:\n","Adapted by Oier Lopez de Lacalle, Olatz Perez de Viñaspre and Ander Barrena, based on a notebook by Sam Bowman at NYU"]},{"cell_type":"code","source":[""],"metadata":{"id":"z9y9f709jJ_W","executionInfo":{"status":"aborted","timestamp":1642100524033,"user_tz":-60,"elapsed":7,"user":{"displayName":"David Cabestany","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13495857735833713912"}}},"execution_count":null,"outputs":[]}]}